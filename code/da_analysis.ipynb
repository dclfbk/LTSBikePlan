{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import osmnx as ox\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import h3\n",
    "from shapely import wkt\n",
    "from shapely.geometry import box, Polygon\n",
    "import folium\n",
    "import os\n",
    "\n",
    "# Load data\n",
    "all_lts_df = pd.read_csv(\"/Users/leonardo/Desktop/Tesi/LTSBikePlan/data/Trento_all_lts.csv\")\n",
    "all_lts_df['geometry'] = all_lts_df['geometry'].apply(wkt.loads)\n",
    "gdf_nodes = pd.read_csv(\"/Users/leonardo/Desktop/Tesi/LTSBikePlan/data/Trento_gdf_nodes.csv\", index_col=0)\n",
    "gdf_nodes['geometry'] = gdf_nodes['geometry'].apply(wkt.loads)\n",
    "\n",
    "#Convert the DataFrame to a GeoDataFrame\n",
    "all_lts = gpd.GeoDataFrame(all_lts_df, geometry='geometry')\n",
    "all_lts.crs = \"EPSG:32632\"\n",
    "all_lts_projected = all_lts.to_crs(epsg=4326)\n",
    "\n",
    "nodes = gpd.GeoDataFrame(gdf_nodes, geometry='geometry')\n",
    "nodes.crs = \"EPSG:32632\"\n",
    "nodes_projected = nodes.to_crs(epsg=4326)\n",
    "\n",
    "def classify_stress(row):\n",
    "    if pd.notna(row['lts']):\n",
    "        if row['lts'] in [1, 2]:\n",
    "            return 'low'\n",
    "        elif row['lts'] in [3, 4]:\n",
    "            return 'high'\n",
    "    return None \n",
    "\n",
    "all_lts_projected['type_stress'] = all_lts_projected.apply(classify_stress, axis=1)\n",
    "nodes_projected['type_stress'] = nodes_projected.apply(classify_stress, axis=1)\n",
    "\n",
    "G = nx.Graph()\n",
    "for idx, row in nodes_projected.iterrows():\n",
    "    G.add_node(idx, **row.to_dict())\n",
    "\n",
    "for _, row in all_lts_projected.iterrows():\n",
    "    u = row['u']\n",
    "    v = row['v']\n",
    "    G.add_edge(u, v, **row.to_dict())\n",
    "\n",
    "base_path = \"/Users/leonardo/Desktop/Tesi/LTSBikePlan/images\"\n",
    "city_name = \"Trento\"\n",
    "city_folder_path = os.path.join(base_path, city_name)\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(city_folder_path):\n",
    "    os.makedirs(city_folder_path)\n",
    "\n",
    "isolated_nodes = list(nx.isolates(G))\n",
    "num_isolated_nodes = len(isolated_nodes)\n",
    "G.remove_nodes_from(isolated_nodes)\n",
    "\n",
    "# Choose a layout\n",
    "pos = {node: (data['geometry'].x, data['geometry'].y) for node, data in G.nodes(data=True)}\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Draw with different colors for low and high stress\n",
    "node_color = ['green' if data['type_stress'] == 'low' else 'red' for node, data in G.nodes(data=True)]\n",
    "edge_color = ['green' if data['type_stress'] == 'low' else 'red' for u, v, data in G.edges(data=True)]\n",
    "nx.draw_networkx_nodes(G, pos, node_color=node_color, node_size=5, alpha=0.8)\n",
    "nx.draw_networkx_edges(G, pos, edge_color=edge_color, alpha=0.5)\n",
    "plt.title(\"Bike Network in Trento\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Create subgraphs based on 'type_stress'\n",
    "low_stress_edges = [(u, v) for u, v, d in G.edges(data=True) if d['type_stress'] == 'low']\n",
    "high_stress_edges = [(u, v) for u, v, d in G.edges(data=True) if d['type_stress'] == 'high']\n",
    "\n",
    "low_stress_subgraph = G.edge_subgraph(low_stress_edges).copy()\n",
    "high_stress_subgraph = G.edge_subgraph(high_stress_edges).copy()\n",
    "\n",
    "# Set up the matplotlib figure with two subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))\n",
    "\n",
    "# Low stress subplot\n",
    "plt.subplot(1, 2, 1)\n",
    "nx.draw_networkx_nodes(low_stress_subgraph, pos, node_size=5, alpha=0.8)\n",
    "nx.draw_networkx_edges(low_stress_subgraph, pos, alpha=0.5)\n",
    "plt.title(\"Low Stress Bike Network in Trento\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "\n",
    "# High stress subplot\n",
    "plt.subplot(1, 2, 2)\n",
    "nx.draw_networkx_nodes(high_stress_subgraph, pos, node_size=5, alpha=0.8)\n",
    "nx.draw_networkx_edges(high_stress_subgraph, pos, alpha=0.5)\n",
    "plt.title(\"High Stress Bike Network in Trento\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve population data from OpenStreetMap\n",
    "def get_osm_population(city_name):\n",
    "    query = f\"\"\"\n",
    "    [out:xml][timeout:25];\n",
    "    area[name=\"{city_name}\"]->.searchArea;\n",
    "    (\n",
    "      node[\"population\"](area.searchArea);\n",
    "      way[\"population\"](area.searchArea);\n",
    "      relation[\"population\"](area.searchArea);\n",
    "    );\n",
    "    out body;\n",
    "    \"\"\"\n",
    "    url = \"https://overpass-api.de/api/interpreter\"\n",
    "    response = requests.get(url, params={'data': query})\n",
    "    root = ET.fromstring(response.content)\n",
    "    # Extract population data\n",
    "    for element in root.iter('tag'):\n",
    "        if element.attrib['k'] == 'population':\n",
    "            return element.attrib['v']\n",
    "    return \"Population data not found\"\n",
    "\n",
    "total_city_population = int(get_osm_population(\"Trento\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.ops import unary_union\n",
    "print(\"Starting process to generate city boundary...\")\n",
    "print(\"Simplifying geometries...\")\n",
    "simplified_geometries = all_lts_projected.geometry.simplify(tolerance=0.001)\n",
    "print(\"Performing unary union on simplified geometries...\")\n",
    "simplified_union = unary_union(simplified_geometries)\n",
    "print(\"Applying buffer...\")\n",
    "city_boundary = simplified_union.buffer(0.005)\n",
    "print(\"City boundary generation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../data/trento_city_boundary.wkt\", \"w\") as file:\n",
    "#     file.write(str(city_boundary))\n",
    "\n",
    "from shapely.wkt import loads\n",
    "\n",
    "with open(\"../data/trento_city_boundary.wkt\", \"r\") as file:\n",
    "    city_boundary = loads(file.read())\n",
    "\n",
    "geo_file_path = '../data/kontur_population_IT_20220630.gpkg'\n",
    "\n",
    "pop = gpd.read_file(geo_file_path)\n",
    "pop = pop.to_crs(\"EPSG:4326\")\n",
    "pop_gdf = pop[pop.geometry.intersects(city_boundary)]\n",
    "print(pop_gdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a hexagonal grid within the city boundaries\n",
    "def create_hex_grid_within_city_bounds(city_boundary, resolution):\n",
    "    hexagons = h3.polyfill(city_boundary.__geo_interface__, resolution, geo_json_conformant=True)\n",
    "\n",
    "    filtered_hexagons = []\n",
    "    for h in hexagons:\n",
    "        hex_polygon = Polygon(h3.h3_to_geo_boundary(h, geo_json=True))\n",
    "        if hex_polygon.intersects(city_boundary):\n",
    "            filtered_hexagons.append(hex_polygon)\n",
    "\n",
    "    hex_grid = gpd.GeoDataFrame([{'geometry': hexagon} for hexagon in filtered_hexagons])\n",
    "    hex_grid.crs = 'EPSG:4326'\n",
    "\n",
    "    return hex_grid\n",
    "\n",
    "hex_grid_within_city = create_hex_grid_within_city_bounds(city_boundary, 9)\n",
    "hex_grid_projected = hex_grid_within_city.to_crs('EPSG: 32632')\n",
    "\n",
    "# # Calculate area for each hexagon to distribute population\n",
    "# hex_grid_projected['area'] = hex_grid_projected['geometry'].area\n",
    "# # Distribute the city's population across the hexagons\n",
    "# hex_grid_projected['estimated_population'] = (hex_grid_projected['area'] / hex_grid_projected['area'].sum()) * total_city_population\n",
    "pop_gdf = pop_gdf.to_crs(hex_grid_projected.crs)\n",
    "\n",
    "# Spatial join\n",
    "joined_gdf = gpd.sjoin(hex_grid_projected, pop_gdf, how=\"left\", predicate='intersects')\n",
    "\n",
    "# Since one hexagon might intersect with multiple population polygons,\n",
    "# we aggregate the population data for each hexagon.\n",
    "# Here we sum the populations, but you might choose a different method\n",
    "# depending on your specific requirements.\n",
    "hex_grid_with_population = joined_gdf.groupby(joined_gdf.index).agg({'population': 'sum'})\n",
    "hex_grid_projected = hex_grid_projected.merge(hex_grid_with_population, left_index=True, right_index=True)\n",
    "hex_grid_projected['population'] = hex_grid_projected['population'].fillna(0)\n",
    "hex_grid_projected = hex_grid_projected.to_crs('EPSG:4326')\n",
    "projected_crs = 'EPSG:32632' \n",
    "all_lts_projected_crs = all_lts_projected.to_crs(projected_crs)\n",
    "\n",
    "# Calculate centroids\n",
    "centroids = all_lts_projected_crs.geometry.centroid\n",
    "centroids = centroids.to_crs(epsg=4326)\n",
    "\n",
    "# Get the center latitude and longitude for the map\n",
    "center_lat, center_lon = centroids.iloc[0].y, centroids.iloc[0].x\n",
    "hex_grid_geojson = hex_grid_projected.to_json()\n",
    "\n",
    "# Function to retrieve building data \n",
    "def get_building_data(city_name):\n",
    "    # Retrieve buildings from OSM within the city boundary\n",
    "    buildings = ox.features_from_place(city_name, tags={'building': True})\n",
    "    return buildings\n",
    "\n",
    "# Retrieve building data for city\n",
    "buildings = get_building_data(\"Trento, Italy\")\n",
    "buildings = buildings.to_crs(hex_grid_projected.crs)\n",
    "hex_grid_projected.reset_index(inplace=True)\n",
    "hex_grid_projected.rename(columns={'index': 'hex_index'}, inplace=True)\n",
    "hex_grid_with_buildings = gpd.sjoin(hex_grid_projected, buildings, how='left', predicate='intersects')\n",
    "building_counts = hex_grid_with_buildings.groupby('hex_index').size()\n",
    "hex_grid_projected['building_count'] = hex_grid_projected['hex_index'].map(building_counts).fillna(0)\n",
    "\n",
    "# Adjust population estimation based on building count: here => simple average of area-based and building-based estimates\n",
    "# hex_grid_projected['adjusted_population'] = hex_grid_projected.apply(\n",
    "#     lambda row: (row['population'] + row['building_count']) / 2, axis=1)\n",
    "# assert 'adjusted_population' in hex_grid_projected.columns, \"Adjusted population column not found\"\n",
    "\n",
    "hex_grid_geojson = hex_grid_projected.to_crs(epsg=4326).to_json()\n",
    "\n",
    "m = folium.Map(location=[center_lat, center_lon], zoom_start=11)\n",
    "\n",
    "def color_function(feature):\n",
    "    population = feature['properties']['population']\n",
    "    \n",
    "    high_population_threshold = total_city_population * 0.02  # 2% of total population\n",
    "    medium_population_threshold = total_city_population * 0.01  # 1% of total population\n",
    "\n",
    "    if population > high_population_threshold:\n",
    "        return '#ff0000'  # Red \n",
    "    elif population > medium_population_threshold:\n",
    "        return '#ffff00'  # Yellow \n",
    "    else:\n",
    "        return '#00ff00'  # Green \n",
    "\n",
    "folium.GeoJson(\n",
    "    hex_grid_geojson,\n",
    "    name='Hexagonal Grid',\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': color_function(feature),\n",
    "        'color': 'black',\n",
    "        'weight': 1,\n",
    "        'fillOpacity': 0.5,\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "# Add layer control and display the map\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(city_folder_path, 'hexagonal_grid_population.html')\n",
    "\n",
    "m.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial join between hexagons and edges\n",
    "edges_gdf = gpd.GeoDataFrame(all_lts_projected[['geometry', 'u', 'v', 'type_stress']], geometry='geometry')\n",
    "hex_edges = gpd.sjoin(hex_grid_projected, edges_gdf, how='left', predicate='intersects')\n",
    "# Group edges by hexagon using a list for column names\n",
    "hexagon_edges = hex_edges.groupby('hex_index')[['u', 'v']].apply(lambda x: list(zip(x['u'], x['v'])))\n",
    "# Remove hexagons with only NaN values in their edge list\n",
    "filtered_hexagon_edges = {hex_id: edges for hex_id, edges in hexagon_edges.items() if not all(np.isnan(edge).any() for edge in edges)}\n",
    "\n",
    "# Filtering is done by checking 'type_stress' for 'low'\n",
    "hexagon_edges_low_stress = hex_edges[hex_edges['type_stress'] == 'low'].groupby('hex_index')[['u', 'v']].apply(lambda x: list(zip(x['u'], x['v'])))\n",
    "\n",
    "# Remove hexagons with only NaN values in their edge list\n",
    "filtered_hexagon_edges_low_stress = {hex_id: edges for hex_id, edges in hexagon_edges_low_stress.items() if not all(np.isnan(edge).any() for edge in edges)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prova shortests path:\n",
    "from scipy.spatial import cKDTree\n",
    "import networkx as nx\n",
    "from shapely.geometry import Point\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import json\n",
    "\n",
    "\n",
    "def find_hexagon_centroid(hexagon_geometry):\n",
    "    return hexagon_geometry.centroid\n",
    "\n",
    "# Precompute node points\n",
    "node_points = {node: Point(data['x'], data['y']) for node, data in G.nodes(data=True)}\n",
    "node_tree = cKDTree([(point.x, point.y) for point in node_points.values()])\n",
    "\n",
    "# Function to find the closest node to a given point\n",
    "def find_closest_node(centroid):\n",
    "    _, nearest_node_index = node_tree.query((centroid.x, centroid.y))\n",
    "    return list(node_points.keys())[nearest_node_index]\n",
    "\n",
    "# Finding representative nodes for each hexagon\n",
    "representative_nodes = {}\n",
    "for hex_id in filtered_hexagon_edges_low_stress:\n",
    "    representative_node = find_closest_node(find_hexagon_centroid(hex_grid_projected.loc[hex_grid_projected['hex_index'] == hex_id, 'geometry'].iloc[0]))\n",
    "    representative_nodes[hex_id] = representative_node\n",
    "\n",
    "# Check if the low-stress subgraph is connected\n",
    "is_connected = nx.is_connected(low_stress_subgraph)\n",
    "print(f\"Is the low-stress subgraph connected? {is_connected}\")\n",
    "\n",
    "# If not connected, find out how many connected components it has\n",
    "connected_components = list(nx.connected_components(low_stress_subgraph)) if not is_connected else []\n",
    "\n",
    "# Count representative nodes in the low-stress subgraph\n",
    "existing_representative_nodes = {hex_id: node for hex_id, node in representative_nodes.items() if node in low_stress_subgraph}\n",
    "num_existing_representative_nodes = len(existing_representative_nodes)\n",
    "total_representative_nodes = len(representative_nodes)\n",
    "print(f\"Number of representative nodes in low-stress subgraph: {num_existing_representative_nodes} out of {total_representative_nodes}\")\n",
    "\n",
    "# Function to check if two nodes are in the same connected component\n",
    "def in_same_component(node1, node2, components):\n",
    "    for component in components:\n",
    "        if node1 in component and node2 in component:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Function to compute shortest paths in low-stress network\n",
    "def compute_shortest_paths_low_stress(start_hex_id, connected_components):\n",
    "    paths = {}\n",
    "    start_node = representative_nodes[start_hex_id]\n",
    "    for target_hex_id, target_node in representative_nodes.items():\n",
    "        if start_hex_id != target_hex_id and in_same_component(start_node, target_node, connected_components):\n",
    "            if nx.has_path(low_stress_subgraph, start_node, target_node):\n",
    "                path_length = nx.dijkstra_path_length(low_stress_subgraph, source=start_node, target=target_node, weight='length')\n",
    "                paths[target_hex_id] = path_length\n",
    "            else:\n",
    "                paths[target_hex_id] = None\n",
    "    return start_hex_id, paths\n",
    "\n",
    "# Start parallel computation of shortest low-stress paths\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(compute_shortest_paths_low_stress, hex_id, connected_components) for hex_id in representative_nodes]\n",
    "    shortest_low_stress_paths = {future.result()[0]: future.result()[1] for future in futures}\n",
    "    executor.shutdown()\n",
    "\n",
    "# Save the shortest_low_stress_paths result to a JSON file\n",
    "file_path = '../data/shortest_low_stress_paths.json'\n",
    "\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(shortest_low_stress_paths, file)\n",
    "\n",
    "shortest_low_stress_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "import networkx as nx\n",
    "from shapely.geometry import Point\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def find_hexagon_centroid(hexagon_geometry):\n",
    "    return hexagon_geometry.centroid\n",
    "\n",
    "print(\"Precomputing node geometries...\")\n",
    "node_points = {node: Point(data['x'], data['y']) for node, data in G.nodes(data=True)}\n",
    "\n",
    "print(\"Creating a spatial index for node points...\")\n",
    "node_tree = cKDTree([(point.x, point.y) for point in node_points.values()])\n",
    "\n",
    "def find_closest_node(centroid):\n",
    "    _, nearest_node_index = node_tree.query((centroid.x, centroid.y))\n",
    "    return list(node_points.keys())[nearest_node_index]\n",
    "\n",
    "print(\"Finding the representative node for each hexagon...\")\n",
    "representative_nodes = {}\n",
    "for hex_id in filtered_hexagon_edges:\n",
    "    representative_node = find_closest_node(find_hexagon_centroid(hex_grid_projected.loc[hex_grid_projected['hex_index'] == hex_id, 'geometry'].iloc[0]))\n",
    "    representative_nodes[hex_id] = representative_node\n",
    "    print(f\"Processed hexagon {hex_id}, found representative node: {representative_node}\")\n",
    "\n",
    "# Function to compute shortest paths in parallel\n",
    "def compute_shortest_paths(start_hex_id):\n",
    "    print(f\"Computing shortest paths for hexagon {start_hex_id}...\")\n",
    "    paths = {}\n",
    "    start_node = representative_nodes[start_hex_id]\n",
    "    for target_hex_id, target_node in representative_nodes.items():\n",
    "        if start_hex_id != target_hex_id and nx.has_path(G, start_node, target_node): \n",
    "            path_length = nx.dijkstra_path_length(G, source=start_node, target=target_node, weight='length')\n",
    "            paths[target_hex_id] = path_length\n",
    "        else:\n",
    "            paths[target_hex_id] = None\n",
    "    print(f\"Completed shortest paths for hexagon {start_hex_id}\")\n",
    "    return start_hex_id, paths\n",
    "\n",
    "print(\"Starting computation of shortest paths using parallel processing...\")\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(compute_shortest_paths, hex_id) for hex_id in representative_nodes]\n",
    "    shortest_paths = {future.result()[0]: future.result()[1] for future in futures}\n",
    "    executor.shutdown()  \n",
    "\n",
    "print(\"Completed computation of shortest paths.\")\n",
    "shortest_paths\n",
    "\n",
    "# Save the result to a JSON file\n",
    "file_path = '../data/shortest_paths.json'\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(shortest_paths, file)\n",
    "    print(f\"Saved shortest paths to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Path to your JSON files\n",
    "file1_path = '../data/shortest_low_stress_paths.json'\n",
    "file2_path = '../data/shortest_paths.json'\n",
    "\n",
    "# Reading the first JSON file\n",
    "with open(file1_path, 'r') as file:\n",
    "    shortest_low_stress_paths = json.load(file)\n",
    "\n",
    "# Reading the second JSON file\n",
    "with open(file2_path, 'r') as file:\n",
    "    shortest_paths = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 150  # Number of paths to extract\n",
    "first_n_paths = {k: shortest_paths[k] for k in list(shortest_paths)[:N]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hexagon_connections = {}\n",
    "detour_threshold = 1.5\n",
    "\n",
    "for hex_id in shortest_paths:\n",
    "    hexagon_connections[hex_id] = {}\n",
    "\n",
    "    for target_hex_id in shortest_paths[hex_id]:\n",
    "        baseline_path_length = shortest_paths[hex_id][target_hex_id]\n",
    "\n",
    "        # Check if the hex_id and target_hex_id exist in shortest_low_stress_paths\n",
    "        if hex_id in shortest_low_stress_paths and target_hex_id in shortest_low_stress_paths[hex_id]:\n",
    "            low_stress_path_length = shortest_low_stress_paths[hex_id][target_hex_id]\n",
    "        else:\n",
    "            low_stress_path_length = None  # Assign None if not found\n",
    "\n",
    "        # Determine if a connection should be made based on the path lengths\n",
    "        if low_stress_path_length is not None and (baseline_path_length is None or low_stress_path_length <= baseline_path_length * detour_threshold):\n",
    "            hexagon_connections[hex_id][target_hex_id] = True\n",
    "        else:\n",
    "            hexagon_connections[hex_id][target_hex_id] = False\n",
    "\n",
    "# hexagon_connections now contains information about which hexagons are connected via low-stress paths\n",
    "# hexagon_connections\n",
    "\n",
    "# Initialize counters\n",
    "true_count = 0\n",
    "false_count = 0\n",
    "\n",
    "# Iterate through the dictionary\n",
    "for hex_id in hexagon_connections:\n",
    "    for target_hex_id in hexagon_connections[hex_id]:\n",
    "        if hexagon_connections[hex_id][target_hex_id]:\n",
    "            true_count += 1\n",
    "        else:\n",
    "            false_count += 1\n",
    "\n",
    "print(\"Number of True connections:\", true_count)\n",
    "print(\"Number of False connections:\", false_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_osm_building_types(city_name, building_types):\n",
    "    all_destinations = []\n",
    "    for building_type in building_types:\n",
    "        try:\n",
    "            query = {'building': building_type}\n",
    "            buildings = ox.features_from_place(city_name, tags=query)\n",
    "\n",
    "            for idx, building in buildings.iterrows():\n",
    "                destination = {\n",
    "                    'name': building.get('name', 'unknown'),\n",
    "                    'type': building_type,\n",
    "                    'coordinates': (building.geometry.centroid.y, building.geometry.centroid.x)\n",
    "                }\n",
    "                all_destinations.append(destination)\n",
    "        except Exception as e:\n",
    "            print(f\"Error querying building type '{building_type}': {e}\")\n",
    "\n",
    "    return all_destinations\n",
    "\n",
    "def query_osm_leisure_types(city_name, leisure_types):\n",
    "    all_destinations = []\n",
    "    for leisure_type in leisure_types:\n",
    "        try:\n",
    "            query = {'leisure': leisure_type}\n",
    "            leisure_facilities = ox.features_from_place(city_name, tags=query)\n",
    "\n",
    "            for idx, facility in leisure_facilities.iterrows():\n",
    "                destination = {\n",
    "                    'name': facility.get('name', 'unknown'),\n",
    "                    'type': leisure_type,\n",
    "                    'coordinates': (facility.geometry.centroid.y, facility.geometry.centroid.x)\n",
    "                }\n",
    "                all_destinations.append(destination)\n",
    "        except Exception as e:\n",
    "            print(f\"Error querying leisure type '{leisure_type}': {e}\")\n",
    "\n",
    "    return all_destinations\n",
    "\n",
    "def query_osm_shop_types(city_name, shop_types):\n",
    "    all_destinations = []\n",
    "    for shop_type in shop_types:\n",
    "        try:\n",
    "            query = {'shop': shop_type}\n",
    "            shops = ox.features_from_place(city_name, tags=query)\n",
    "\n",
    "            for idx, shop in shops.iterrows():\n",
    "                destination = {\n",
    "                    'name': shop.get('name', 'unknown'),\n",
    "                    'type': shop_type,\n",
    "                    'coordinates': (shop.geometry.centroid.y, shop.geometry.centroid.x)\n",
    "                }\n",
    "                all_destinations.append(destination)\n",
    "        except Exception as e:\n",
    "            print(f\"Error querying shop type '{shop_type}': {e}\")\n",
    "\n",
    "    return all_destinations\n",
    "\n",
    "def query_osm_bus_stops(city_name):\n",
    "    all_bus_stops = []\n",
    "    try:\n",
    "        query = {'highway': 'bus_stop'}\n",
    "        bus_stops = ox.features_from_place(city_name, tags=query)\n",
    "\n",
    "        for idx, bus_stop in bus_stops.iterrows():\n",
    "            name = str(bus_stop.get('name', 'Unknown')).replace('/', '_').replace('\"', '')\n",
    "            stop = {\n",
    "                'name': name,\n",
    "                'type': \"bus_stop\",\n",
    "                'coordinates': (bus_stop.geometry.centroid.y, bus_stop.geometry.centroid.x)\n",
    "            }\n",
    "            all_bus_stops.append(stop)\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying bus stops: {e}\")\n",
    "\n",
    "    return all_bus_stops\n",
    "\n",
    "# Use\n",
    "city_name = \"Trento, Italy\"\n",
    "shop_types = ['mall']\n",
    "leisure_types = ['sports_centre', 'stadium', 'park', 'fitness_centre']\n",
    "building_types = ['sports_centre', 'train_station', 'supermarket', 'bus_station', 'fitness_centre', 'mall', 'retail', 'shop']\n",
    "destinations_shop = query_osm_shop_types(city_name, shop_types)\n",
    "destinations_lei = query_osm_leisure_types(city_name, leisure_types)\n",
    "destinations_build = query_osm_building_types(city_name, building_types)\n",
    "#bus_stops = query_osm_bus_stops(city_name)\n",
    "\n",
    "combined_destinations = destinations_shop + destinations_lei + destinations_build #+ bus_stops\n",
    "unique_coordinates = set()\n",
    "dsnt = []\n",
    "for destination in combined_destinations:\n",
    "    coords = destination['coordinates']\n",
    "    if coords not in unique_coordinates:\n",
    "        dsnt.append(destination)\n",
    "        unique_coordinates.add(coords)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the destinations category data\n",
    "with open('destinations.json') as f:\n",
    "    destinations_data = json.load(f)\n",
    "\n",
    "# Extracting destination types\n",
    "destination_types = []\n",
    "for category in destinations_data['categories']:\n",
    "    for dest_type in category['types']:\n",
    "        destination_types.append(dest_type['type_name'])\n",
    "\n",
    "def query_osm_for_destinations(city_name, dest_types):\n",
    "    all_destinations = []\n",
    "    for dest_type in dest_types:\n",
    "        try:\n",
    "            # Initially query with 'amenity' tag\n",
    "            destinations = ox.features_from_place(city_name, tags={'amenity': dest_type})\n",
    "            if not destinations.empty:\n",
    "                for idx, row in destinations.iterrows():\n",
    "                    all_destinations.append({\n",
    "                        'name': row.get('name', 'Unknown'),\n",
    "                        'type': dest_type,\n",
    "                        'coordinates': (row.geometry.centroid.y, row.geometry.centroid.x)\n",
    "                    })\n",
    "            else:\n",
    "                print(f\"No data returned for destination type: {dest_type}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error querying {dest_type}: {e}\")\n",
    "\n",
    "    return all_destinations\n",
    "\n",
    "list_destinations = query_osm_for_destinations(city_name, destination_types)\n",
    "list_destinations = dsnt + list_destinations\n",
    "list_destinations\n",
    "\n",
    "# Initialize an empty set to track unique types\n",
    "unique_types = set()\n",
    "\n",
    "# Iterate through the dsnt list and collect unique types\n",
    "for destination in list_destinations:\n",
    "    unique_types.add(destination['type'])\n",
    "\n",
    "# # Print the distinct types\n",
    "# print(\"Distinct Types in list destinations:\")\n",
    "# for t in unique_types:\n",
    "#     print(t)\n",
    "\n",
    "# Associate destinations with hexagons\n",
    "points_geometry = [Point(coord[1], coord[0]) for coord in [destination['coordinates'] for destination in list_destinations]]\n",
    "gdf_destinations = gpd.GeoDataFrame(list_destinations, geometry=points_geometry)\n",
    "gdf_destinations.crs = 'EPSG:4326'\n",
    "\n",
    "# Ensuring CRS match\n",
    "gdf_destinations = gdf_destinations.to_crs(hex_grid_projected.crs)\n",
    "#print(gdf_destinations)\n",
    "# Spatial join\n",
    "joined_df = gpd.sjoin(hex_grid_projected, gdf_destinations, how='left', predicate='contains')\n",
    "print(joined_df)\n",
    "\n",
    "# Check for NaN values in the specified columns\n",
    "nan_check = joined_df[['index_right', 'name', 'type', 'coordinates']].isna().all(axis=1).sum()\n",
    "#print(f\"\\nNumber of rows where 'index_right', 'name', 'type', 'coordinates' are all NaN: {nan_check}\")\n",
    "\n",
    "# Checking for non-NaN values\n",
    "not_nan_check = joined_df[['index_right', 'name', 'type', 'coordinates']].dropna().shape[0]\n",
    "#print(f\"\\nNumber of rows with valid data in 'index_right', 'name', 'type', 'coordinates': {not_nan_check}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a row represents a destination hexagon\n",
    "def is_destination(row):\n",
    "    return not pd.isna(row.get('index_right')) and not pd.isna(row.get('name')) and not pd.isna(row.get('type')) and not pd.isna(row.get('coordinates'))\n",
    "\n",
    "# Identify destination hexagons\n",
    "joined_df['centroid'] = joined_df.geometry.centroid\n",
    "destination_hexagons = joined_df[joined_df.apply(is_destination, axis=1)]\n",
    "\n",
    "# Reproject to a suitable projected CRS\n",
    "#projected_crs = 'EPSG:32632'  \n",
    "joined_df = joined_df.to_crs(projected_crs)\n",
    "\n",
    "\n",
    "print(\"Sample Destination Hexagons:\")\n",
    "print(destination_hexagons.head())\n",
    "#print(joined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyproj\n",
    "from shapely.ops import transform\n",
    "from shapely.geometry import Point\n",
    "import networkx as nx\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# Cache for CRS transformers\n",
    "transformers_cache = {}\n",
    "\n",
    "def get_transformer(point_crs, graph_crs):\n",
    "    if (point_crs, graph_crs) not in transformers_cache:\n",
    "        point_proj = pyproj.CRS(point_crs)\n",
    "        graph_proj = pyproj.CRS(graph_crs)\n",
    "        transformers_cache[(point_crs, graph_crs)] = pyproj.Transformer.from_crs(point_proj, graph_proj, always_xy=True).transform\n",
    "    return transformers_cache[(point_crs, graph_crs)]\n",
    "\n",
    "def transform_point_to_graph_crs(point, point_crs, graph_crs):\n",
    "    project = get_transformer(point_crs, graph_crs)\n",
    "    return transform(project, point)\n",
    "\n",
    "def build_spatial_index(G):\n",
    "    points = [(data['x'], data['y']) for _, data in G.nodes(data=True)]\n",
    "    return cKDTree(points), points\n",
    "\n",
    "def nearest_node(G, point, spatial_index, point_crs='EPSG:32632', graph_crs='EPSG:4326'):\n",
    "    transformed_point = transform_point_to_graph_crs(point, point_crs, graph_crs)\n",
    "    nearest_idx = spatial_index.query(transformed_point.coords[0])[1]\n",
    "    nearest_node_id = list(G.nodes)[nearest_idx]\n",
    "    return nearest_node_id\n",
    "\n",
    "def dijkstra_distance(G, source_geometry, target_geometry, spatial_index, point_crs='EPSG:32632', graph_crs='EPSG:4326'):\n",
    "    source_node = nearest_node(G, source_geometry, spatial_index, point_crs, graph_crs)\n",
    "    target_node = nearest_node(G, target_geometry, spatial_index, point_crs, graph_crs)\n",
    "    try:\n",
    "        distance = nx.dijkstra_path_length(G, source=source_node, target=target_node, weight='length')\n",
    "    except nx.NetworkXNoPath:\n",
    "        distance = float('inf')\n",
    "    return distance\n",
    "\n",
    "# Pre-processing step\n",
    "spatial_index, _ = build_spatial_index(G)\n",
    "\n",
    "# Main loop for calculating accessible destinations\n",
    "hexagon_destinations_1 = {}\n",
    "for index, hexagon in joined_df[:400].iterrows():\n",
    "    print(f\"Processing hexagon at index {index}\")\n",
    "    accessible_destinations = []\n",
    "    for dest_index, destination in destination_hexagons.iterrows():\n",
    "        if 'centroid' in hexagon and 'centroid' in destination:\n",
    "            distance = dijkstra_distance(G, hexagon['centroid'], destination['centroid'], spatial_index)\n",
    "            if distance <= 2000:\n",
    "                accessible_destinations.append((destination['name'], destination['type'], destination['geometry'], dest_index))\n",
    "\n",
    "    hexagon_destinations_1[(index, hexagon['geometry'])] = {\n",
    "        'total_number_of_destinations': len(accessible_destinations),\n",
    "        'destinations': accessible_destinations\n",
    "    }\n",
    "    print(f\"Processed hexagon at index {index}\")\n",
    "\n",
    "print(\"Processing complete. Hexagon destinations calculated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hexagon_destinations_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hexagon_destinations_2 = {}\n",
    "for index, hexagon in joined_df[400:801].iterrows():\n",
    "    print(f\"Processing hexagon at index {index}\")\n",
    "    accessible_destinations = []\n",
    "    for dest_index, destination in destination_hexagons.iterrows():\n",
    "        if 'centroid' in hexagon and 'centroid' in destination:\n",
    "            distance = dijkstra_distance(G, hexagon['centroid'], destination['centroid'], spatial_index)\n",
    "            if distance <= 2000:\n",
    "                accessible_destinations.append((destination['name'], destination['type'], destination['geometry'], dest_index))\n",
    "\n",
    "    hexagon_destinations_2[(index, hexagon['geometry'])] = {\n",
    "        'total_number_of_destinations': len(accessible_destinations),\n",
    "        'destinations': accessible_destinations\n",
    "    }\n",
    "    print(f\"Processed hexagon at index {index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hexagon_destinations_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hexagon_destinations_3 = {}\n",
    "for index, hexagon in joined_df[801:].iterrows():\n",
    "    print(f\"Processing hexagon at index {index}\")\n",
    "    accessible_destinations = []\n",
    "    for dest_index, destination in destination_hexagons.iterrows():\n",
    "        if 'centroid' in hexagon and 'centroid' in destination:\n",
    "            distance = dijkstra_distance(G, hexagon['centroid'], destination['centroid'], spatial_index)\n",
    "            if distance <= 2000:\n",
    "                accessible_destinations.append((destination['name'], destination['type'], destination['geometry'], dest_index))\n",
    "\n",
    "    hexagon_destinations_3[(index, hexagon['geometry'])] = {\n",
    "        'total_number_of_destinations': len(accessible_destinations),\n",
    "        'destinations': accessible_destinations\n",
    "    }\n",
    "    print(f\"Processed hexagon at index {index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hexagon_destinations_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hexagon_destinations = {**hexagon_destinations_1, **hexagon_destinations_2,**hexagon_destinations_3}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hexagon_destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict_to_file(dict_data, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for key, value in dict_data.items():\n",
    "            file.write(f\"{key}: {value}\\n\")\n",
    "        print(f\"Data saved to {file_path}\")\n",
    "\n",
    "file_path = \"../data/hexagon_destinations_shortest_path.txt\" \n",
    "save_dict_to_file(hexagon_destinations, file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_index, _ = build_spatial_index(low_stress_subgraph)\n",
    "\n",
    "# Second loop for G_low_stress graph\n",
    "for index, hexagon in joined_df.iterrows():\n",
    "    accessible_destinations_low = []\n",
    "    for dest_index, destination in destination_hexagons.iterrows():\n",
    "        # Ensure that centroid exists for both hexagon and destination\n",
    "        if 'centroid' in hexagon and 'centroid' in destination:\n",
    "            distance = dijkstra_distance(low_stress_subgraph, hexagon['centroid'], destination['centroid'], spatial_index)\n",
    "\n",
    "            if distance <= 2000: \n",
    "                accessible_destinations_low.append((destination['name'], destination['type'], destination['geometry'], dest_index))\n",
    "\n",
    "    # Update the existing hexagon_destinations dictionary\n",
    "    hexagon_destinations[(index, hexagon['geometry'])].update({\n",
    "        'total_number_of_destinations_low': len(accessible_destinations_low),\n",
    "        'destinations_low': accessible_destinations_low\n",
    "    })\n",
    "    print(f\"Processed hexagon at index {index}\")\n",
    "\n",
    "hexagon_destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scoring processes\n",
    "with open('bna_scoring.json') as file:\n",
    "    scoring_data = json.load(file)\n",
    "\n",
    "# Convert the JSON data into more accessible structures\n",
    "destination_weights = {category['category_name']: category['weight'] for category in destinations_data['categories']}\n",
    "type_weights = {dest_type['type_name']: dest_type['weight'] for category in destinations_data['categories'] for dest_type in category['types']}\n",
    "scoring_processes = {process['process']: process['criteria'] for process in scoring_data['scoring_process']}\n",
    "\n",
    "# Function to calculate score for a single destination type\n",
    "def calculate_score_for_type(destinations, scoring_process):\n",
    "    criteria = scoring_processes[scoring_process]\n",
    "    score = 0\n",
    "    for i, destination in enumerate(destinations):\n",
    "        if i < len(criteria):\n",
    "            score += criteria[i]['points']\n",
    "    return score\n",
    "\n",
    "# Score each hexagon\n",
    "for hexagon, hex_data in hexagon_destinations.items():\n",
    "    hex_scores = {}\n",
    "    for category in destinations_data['categories']:\n",
    "        cat_score = 0\n",
    "        for dest_type in category['types']:\n",
    "            type_name = dest_type['type_name']\n",
    "            scoring_proc = dest_type['scoring_process']\n",
    "            destinations = [d for d in hex_data['destinations'] if d[1] == type_name]\n",
    "            type_score = calculate_score_for_type(destinations, scoring_proc)\n",
    "            weighted_type_score = type_score * type_weights[type_name] / 100\n",
    "            cat_score += weighted_type_score\n",
    "\n",
    "        # Apply category weight\n",
    "        weighted_cat_score = cat_score * destination_weights[category['category_name']] / 100\n",
    "        hex_scores[category['category_name']] = weighted_cat_score\n",
    "\n",
    "    hexagon_destinations[hexagon]['scores'] = hex_scores\n",
    "\n",
    "for hexagon in hexagon_destinations:\n",
    "    total_score = sum(hexagon_destinations[hexagon]['scores'].values())\n",
    "    hexagon_destinations[hexagon]['total_score'] = total_score\n",
    "\n",
    "# Iterate over hexagons to calculate total scores and overall score\n",
    "overall_score = 0\n",
    "total_population = sum(joined_df['population'])\n",
    "\n",
    "for index, hexagon in joined_df.iterrows():\n",
    "    hex_population = hexagon['population']\n",
    "    hex_key = (index, hexagon['geometry'])\n",
    "    hex_data = hexagon_destinations.get(hex_key, {})\n",
    "    hex_score = hex_data.get('total_score', 0)  # Get the total score for the hexagon\n",
    "\n",
    "    # Weight the hexagon's score by its population and add to overall score\n",
    "    weighted_score = hex_score * (hex_population / total_population)\n",
    "    overall_score += weighted_score\n",
    "\n",
    "# Print the final overall score\n",
    "print(f\"Overall Score: {overall_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting necessary data\n",
    "data = []\n",
    "for (index, polygon), details in hexagon_destinations.items():\n",
    "    total_score = details['total_score']\n",
    "    data.append((index, polygon, total_score))\n",
    "\n",
    "# Creating DataFrame\n",
    "df = pd.DataFrame(data, columns=['index', 'geometry', 'total_score'])\n",
    "\n",
    "# Setting the index\n",
    "df.set_index('index', inplace=True)\n",
    "\n",
    "# Converting to GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "\n",
    "# Setting up the correct CRS if needed\n",
    "gdf.crs = \"EPSG:4326\"\n",
    "\n",
    "# The result is a GeoDataFrame named hex_totalscore\n",
    "hex_totalscore = gdf\n",
    "hex_totalscore\n",
    "\n",
    "# Specify the file path and name\n",
    "file_path = '../data/hex_totalscore.shp'\n",
    "\n",
    "# Save the GeoDataFrame as a Shapefile\n",
    "hex_totalscore.to_file(file_path)\n",
    "\n",
    "# Read the Shapefile into a GeoDataFrame\n",
    "loaded_gdf = gpd.read_file(file_path)\n",
    "\n",
    "# Set the 'index' column as the index of the GeoDataFrame\n",
    "loaded_gdf.set_index('index', inplace=True)\n",
    "\n",
    "# Display the loaded GeoDataFrame to check its contents\n",
    "print(loaded_gdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "loaded_gdf['rounded_score'] = loaded_gdf['total_scor'].round().astype(int)\n",
    "\n",
    "# Group by the rounded_score and count the number of hexagons per score\n",
    "hexagon_count = loaded_gdf.groupby('rounded_score').size().reset_index(name='nr_hexagons')\n",
    "descriptive_stats = loaded_gdf['rounded_score'].describe()\n",
    "\n",
    "# Using seaborn to improve the visualization\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x='rounded_score', y='nr_hexagons', data=hexagon_count, palette='viridis')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Distribution of Hexagons for Rounded Scores', fontsize=16)\n",
    "plt.xlabel('Rounded Total Score', fontsize=14)\n",
    "plt.ylabel('Number of Hexagons', fontsize=14)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "sns.despine()\n",
    "\n",
    "file_path = os.path.join(city_folder_path, 'distribution_totalscores_plot.png')\n",
    "plt.savefig(file_path)\n",
    "# Show the plot\n",
    "plt.show()\n",
    "print(descriptive_stats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import branca.colormap as cm\n",
    "\n",
    "hex_dest_data = [{'geometry': hex_geom, 'total_score': hex_data['total_score']} \n",
    "                 for (_, hex_geom), hex_data in hexagon_destinations.items()]\n",
    "\n",
    "hex_dest_gdf = gpd.GeoDataFrame(hex_dest_data, crs='EPSG:32632')\n",
    "\n",
    "hex_dest_gdf_projected = hex_dest_gdf.to_crs('EPSG:4326')\n",
    "center_lat = hex_dest_gdf_projected.geometry.apply(lambda geom: geom.centroid.y).mean()\n",
    "center_lon = hex_dest_gdf_projected.geometry.apply(lambda geom: geom.centroid.x).mean()\n",
    "\n",
    "# Create a base folium map\n",
    "map_da = folium.Map(location=[center_lat, center_lon], zoom_start=11.2)\n",
    "\n",
    "\n",
    "# Function to determine the color of a hexagon based on its total_score\n",
    "def get_hexagon_color(total_score):\n",
    "    # You can adjust the color scheme based on your preference and scoring scale\n",
    "    if total_score > 20:\n",
    "        return 'green'\n",
    "    elif total_score > 10:\n",
    "        return 'orange'\n",
    "    elif total_score > 5:\n",
    "        return 'red'\n",
    "    else:\n",
    "        return 'darkred'\n",
    "\n",
    "legend_colormap = cm.LinearColormap(\n",
    "    colors=['darkred', 'red', 'orange', 'green'],\n",
    "    index=[0, 5, 10, 20],\n",
    "    vmin=0,\n",
    "    vmax=20,\n",
    "    caption='Total Score'\n",
    ")\n",
    "\n",
    "legend_colormap.add_to(map_da)\n",
    "\n",
    "for _, row in hex_dest_gdf_projected.iterrows():\n",
    "    color = get_hexagon_color(row['total_score'])\n",
    "    folium.GeoJson(\n",
    "        row['geometry'],\n",
    "        style_function=lambda _, color=color: {\n",
    "            'fillColor': color,\n",
    "            'color': 'black',\n",
    "            'weight': 1,\n",
    "            'fillOpacity': 0.7\n",
    "        }\n",
    "    ).add_to(map_da)\n",
    "\n",
    "file_path = os.path.join(city_folder_path, 'bna_score_map.html')\n",
    "\n",
    "# Assuming 'accident_map' is a Folium Map object\n",
    "map_da.save(file_path)\n",
    "\n",
    "map_da"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
