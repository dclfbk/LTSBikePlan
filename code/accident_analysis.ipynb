{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "import os\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point\n",
    "from folium.plugins import HeatMap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# Get the city from command-line arguments\n",
    "#city = os.environ.get('CITY', 'Default City Name')\n",
    "#base_path = \"/Users/leonardo/Desktop/Tesi/LTSBikePlan/data/\"\n",
    "\n",
    "# Sanitize the city name\n",
    "#city_sanitized = city.split(\",\")[0].replace(\" \", \"_\")\n",
    "#city_path = f\"{base_path}{city_sanitized}\"  # Use the sanitized city name here\n",
    "\n",
    "# Load data\n",
    "#all_lts_df = pd.read_csv(f\"{city_path}_all_lts.csv\")\n",
    "all_lts_df = pd.read_csv(\"/Users/leonardo/Desktop/Tesi/LTSBikePlan/data/Trento_all_lts.csv\")\n",
    "# Convert 'geometry' column from WKT strings to actual geometry objects\n",
    "all_lts_df['geometry'] = all_lts_df['geometry'].apply(wkt.loads)\n",
    "\n",
    "# Convert the DataFrame to a GeoDataFrame\n",
    "all_lts = gpd.GeoDataFrame(all_lts_df, geometry='geometry')\n",
    "\n",
    "# Set the CRS for the GeoDataFrame\n",
    "all_lts.crs = \"EPSG:32632\"\n",
    "all_lts_projected = all_lts.to_crs(epsg=4326)\n",
    "\n",
    "#gdf_nodes = pd.read_csv(f\"{city_path}_gdf_nodes.csv\", index_col=0)\n",
    "gdf_nodes = pd.read_csv(\"/Users/leonardo/Desktop/Tesi/LTSBikePlan/data/Trento_gdf_nodes.csv\", index_col=0)\n",
    "gdf_nodes['geometry'] = gdf_nodes['geometry'].apply(wkt.loads)\n",
    "# Convert the DataFrame to a GeoDataFrame\n",
    "gdf_nodes = gpd.GeoDataFrame(gdf_nodes, geometry='geometry')\n",
    "gdf_nodes.crs = \"EPSG:32632\"\n",
    "gdf_nodes_projected = gdf_nodes.to_crs(epsg=4326)\n",
    "\n",
    "accidents_df = gpd.read_file(\"/Users/leonardo/Desktop/Tesi/LTSBikePlan/data/accidents_trento.geojson\")\n",
    "accidents_df = accidents_df.drop(columns=['id'], errors='ignore')\n",
    "accidents_df.reset_index(inplace=True)\n",
    "accidents_df.rename(columns={'index': 'id'}, inplace=True)\n",
    "accidents_df.set_index('id', inplace=True)\n",
    "\n",
    "base_path = \"/Users/leonardo/Desktop/Tesi/LTSBikePlan/images\"\n",
    "city_name = \"Trento\"\n",
    "\n",
    "# Create the path for the new folder\n",
    "city_folder_path = os.path.join(base_path, city_name)\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(city_folder_path):\n",
    "    os.makedirs(city_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the 'geometry' column needs conversion\n",
    "if isinstance(accidents_df.iloc[0]['geometry'], str):\n",
    "    # Convert 'geometry' column from WKT strings to actual geometry objects\n",
    "    accidents_df['geometry'] = accidents_df['geometry'].apply(wkt.loads)\n",
    "elif isinstance(accidents_df.iloc[0]['geometry'], Point):\n",
    "    # If already in Point format, no need to convert\n",
    "    pass\n",
    "else:\n",
    "    raise ValueError(\"Unexpected geometry format in accidents_df\")\n",
    "\n",
    "# Convert the DataFrame to a GeoDataFrame for accidents\n",
    "accidents_gdf = gpd.GeoDataFrame(accidents_df, geometry='geometry')\n",
    "\n",
    "# Set the CRS for the GeoDataFrame to match the original data's CRS\n",
    "accidents_gdf.crs = \"EPSG:25832\"\n",
    "\n",
    "# Reproject the GeoDataFrame to WGS84 for mapping\n",
    "accidents_gdf = accidents_gdf.to_crs(epsg=4326)\n",
    "\n",
    "# Filter for the last 10 years (2013-2023)\n",
    "#accidents_last_10_years = accidents_gdf[accidents_gdf['anno'].between(2013, 2023)]\n",
    "\n",
    "# Filter for the last 5 years (2018-2023)\n",
    "accidents_gdf = accidents_gdf[accidents_gdf['anno'].between(2018, 2023)].copy()\n",
    "\n",
    "# Create a base map using Folium at a central location in your data\n",
    "map_center = [accidents_gdf['geometry'].y.mean(), accidents_gdf['geometry'].x.mean()]\n",
    "accident_map = folium.Map(location=map_center, zoom_start=12)\n",
    "\n",
    "# Add each accident location as a marker on the map\n",
    "for _, row in accidents_gdf.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row.geometry.y, row.geometry.x],\n",
    "        radius=2,\n",
    "        color='red',\n",
    "        fill=True,\n",
    "        fill_color='red',\n",
    "        fill_opacity=0.7\n",
    "    ).add_to(accident_map)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "# Full path for the file\n",
    "file_path = os.path.join(city_folder_path, 'accident_map.html')\n",
    "\n",
    "# Assuming 'accident_map' is a Folium Map object\n",
    "accident_map.save(file_path)\n",
    "\n",
    "# Display the map in Jupyter Notebook (if you are using it)\n",
    "accident_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a base map\n",
    "heatmap_map = folium.Map(location=map_center, zoom_start=12)\n",
    "\n",
    "# Extract coordinates for the heatmap\n",
    "heat_data = [[point.xy[1][0], point.xy[0][0]] for point in accidents_gdf.geometry ]\n",
    "\n",
    "# Create a HeatMap layer and add it to the base map\n",
    "HeatMap(heat_data, \n",
    "        radius=13, \n",
    "        blur=15,\n",
    ").add_to(heatmap_map)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "# Full path for the file\n",
    "file_path = os.path.join(city_folder_path, 'heatmap_map.html')\n",
    "\n",
    "# Assuming 'accident_map' is a Folium Map object\n",
    "heatmap_map.save(file_path)\n",
    "\n",
    "heatmap_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KDE plot\n",
    "\n",
    "x = accidents_gdf.geometry.x\n",
    "y = accidents_gdf.geometry.y\n",
    "\n",
    "# Create a KDE plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(x=x, y=y, shade=True, cmap=\"Reds\")\n",
    "plt.title('Kernel Density Estimation of Accidents')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinates\n",
    "x = accidents_gdf.geometry.x\n",
    "y = accidents_gdf.geometry.y\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.kdeplot(x=x, y=y, shade=True, cmap=\"Reds\")\n",
    "\n",
    "x_min, x_max = accidents_gdf.geometry.x.min(), accidents_gdf.geometry.x.max()\n",
    "y_min, y_max = accidents_gdf.geometry.y.min(), accidents_gdf.geometry.y.max()\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_ylim(y_min, y_max)\n",
    "\n",
    "plt.axis('off')\n",
    "\n",
    "fig = ax.get_figure()\n",
    "# Save the map to an HTML file\n",
    "# Full path for the file\n",
    "file_path = os.path.join(city_folder_path, 'kde_plot.png')\n",
    "\n",
    "# Assuming 'accident_map' is a Folium Map object\n",
    "fig.savefig(file_path, bbox_inches='tight', pad_inches=0, transparent=True)\n",
    "\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium import plugins\n",
    "\n",
    "# Create a base map\n",
    "map_center = [accidents_gdf.geometry.y.mean(), accidents_gdf.geometry.x.mean()]\n",
    "kde_map = folium.Map(location=map_center, zoom_start=12)\n",
    "\n",
    "bounds = [\n",
    "    [accidents_gdf.geometry.y.min(), accidents_gdf.geometry.x.min()],\n",
    "    [accidents_gdf.geometry.y.max(), accidents_gdf.geometry.x.max()]\n",
    "]\n",
    "\n",
    "# Create an ImageOverlay object\n",
    "kde_image = folium.raster_layers.ImageOverlay(\n",
    "    image='/Users/leonardo/Desktop/Tesi/LTSBikePlan/images/Trento/kde_plot.png',\n",
    "    bounds=bounds,\n",
    "    opacity=0.6  # adjust to your preference\n",
    ")\n",
    "\n",
    "kde_image.add_to(kde_map)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "# Full path for the file\n",
    "file_path = os.path.join(city_folder_path, 'kde_map.html')\n",
    "\n",
    "# Assuming 'accident_map' is a Folium Map object\n",
    "kde_map.save(file_path)\n",
    "\n",
    "kde_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "\n",
    "all_lts = all_lts.to_crs(epsg=4326)\n",
    "\n",
    "# Projection\n",
    "accidents_gdf_projected = accidents_gdf.to_crs(epsg=32632)  \n",
    "accidents_gdf_buffered = accidents_gdf_projected.copy()\n",
    "accidents_gdf_buffered['geometry'] = accidents_gdf_projected.geometry.buffer(10)  # Buffer by 10 meters\n",
    "accidents_gdf_buffered = accidents_gdf_buffered.to_crs(epsg=4326)\n",
    "\n",
    "# Perform the spatial join again\n",
    "accidents_near_edges = gpd.sjoin(accidents_gdf_buffered, all_lts, how='left', predicate='intersects')\n",
    "accidents_near_edges = accidents_near_edges[accidents_near_edges['anno'].notna()]\n",
    "accidents_near_edges['anno'] = accidents_near_edges['anno'].astype(int)\n",
    "\n",
    "# Same analysis for nodes\n",
    "gdf_nodes_projected['geometry'] = gdf_nodes_projected.apply(lambda row: Point(row['x'], row['y']), axis=1)\n",
    "accidents_near_nodes = gpd.sjoin(accidents_gdf_buffered, gdf_nodes_projected, how='left', predicate='intersects')\n",
    "accidents_near_nodes = accidents_near_nodes[accidents_near_nodes['anno'].notna()]\n",
    "accidents_near_nodes['anno'] = accidents_near_nodes['anno'].astype(int)\n",
    "\n",
    "#Filter data for the last 10 years\n",
    "# accidents_near_edges = accidents_near_edges[(accidents_near_edges['anno'] >= 2013) & (accidents_near_edges['anno'] <= 2023)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_near_edges['lanes_assumed'] = accidents_near_edges['lanes_assumed'].apply(lambda x: str(int(x)) if pd.notna(x) else x)\n",
    "accidents_near_edges['maxspeed_assumed'] = accidents_near_edges['maxspeed_assumed'].apply(lambda x: str(int(x)) if pd.notna(x) else x)\n",
    "\n",
    "# Fill NaN values in 'lanes' and 'maxspeed' with values from 'lanes_assumed'\n",
    "accidents_near_edges['lanes'] = accidents_near_edges['lanes'].fillna(accidents_near_edges['lanes_assumed'])\n",
    "accidents_near_edges['maxspeed'] = accidents_near_edges['maxspeed'].fillna(accidents_near_edges['maxspeed_assumed'])\n",
    "\n",
    "# Check for NaN values again after filling\n",
    "nan_lanes_after = accidents_near_edges['lanes'].isna().sum()\n",
    "nan_maxspeed_after = accidents_near_edges['maxspeed'].isna().sum()\n",
    "\n",
    "print(f\"NaN values in 'lanes' after filling: {nan_lanes_after}\")\n",
    "print(f\"NaN values in 'maxspeed' after filling: {nan_maxspeed_after}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency of Accidents near different types of roads\n",
    "\n",
    "# Count accidents by road type\n",
    "accidents_by_road_type = accidents_near_edges.groupby('highway').size().reset_index(name='accident_count')\n",
    "accidents_by_road_type.sort_values('accident_count', ascending=False, inplace=True)\n",
    "#print(accidents_by_road_type)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "sns.barplot(data=accidents_by_road_type, x='accident_count', y='highway')\n",
    "plt.xlabel('Accident Count')\n",
    "plt.ylabel('Road Type')\n",
    "plt.title('Frequency of Accidents by Road Type')\n",
    "file_path = os.path.join(city_folder_path, 'frequencyaccidentsbyroads_plot.png')\n",
    "plt.savefig(file_path)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Correlation with number of lanes or maximum speed limits\n",
    "accidents_near_edges['lanes'] = accidents_near_edges['lanes'].fillna(accidents_near_edges['lanes_assumed'])\n",
    "accidents_near_edges['maxspeed'] = accidents_near_edges['maxspeed'].fillna(accidents_near_edges['maxspeed_assumed'])\n",
    "\n",
    "# Aggregate by lanes and count accidents\n",
    "accidents_by_lanes = accidents_near_edges.groupby('lanes').size().reset_index(name='accident_count')\n",
    "\n",
    "def clean_lane_values(lane_value):\n",
    "    lane_numbers = re.findall(r'\\d+', lane_value)\n",
    "    return ' or '.join(lane_numbers) if lane_numbers else lane_value\n",
    "\n",
    "# Apply the cleaning function to the lanes column\n",
    "accidents_by_lanes['lanes'] = accidents_by_lanes['lanes'].apply(clean_lane_values)\n",
    "\n",
    "# Reorder lanes for plotting\n",
    "accidents_by_lanes['lanes'] = pd.Categorical(accidents_by_lanes['lanes'], categories=sorted(set(accidents_by_lanes['lanes']), key=lambda x: float('inf') if x == '' else float(x.split(' or ')[0])))\n",
    "accidents_by_lanes.sort_values('lanes', inplace=True)\n",
    "\n",
    "# Bar plot for accidents by the number of lanes\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=accidents_by_lanes, x='lanes', y='accident_count')\n",
    "plt.xlabel('Number of Lanes')\n",
    "plt.ylabel('Accident Count')\n",
    "plt.title('Accidents by Number of Lanes')\n",
    "plt.xticks(rotation=45)\n",
    "file_path = os.path.join(city_folder_path, 'accidentsbynumberlanes_plot.png')\n",
    "plt.savefig(file_path)\n",
    "plt.show()\n",
    "\n",
    "# Aggregate by max_speed and count accidents\n",
    "accidents_by_maxspeed = accidents_near_edges.groupby('maxspeed').size().reset_index(name='accident_count')\n",
    "\n",
    "def clean_speed_limit(value):\n",
    "    if 'IT:urban' in value:\n",
    "        return 50\n",
    "    else:\n",
    "        speeds = [int(s) for s in re.findall(r'\\d+', value)]\n",
    "        return max(speeds) if speeds else 50  # Default to 50 if no numbers found\n",
    "\n",
    "accidents_by_maxspeed['maxspeed'] = accidents_by_maxspeed['maxspeed'].apply(clean_speed_limit)\n",
    "accidents_by_maxspeed = accidents_by_maxspeed.groupby('maxspeed').agg({'accident_count': 'sum'}).reset_index()\n",
    "\n",
    "# Scatter plot for accidents by maximum speed limits\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=accidents_by_maxspeed, x='maxspeed', y='accident_count', marker='o')\n",
    "plt.xlabel('Maximum Speed Limit (km/h)')\n",
    "plt.ylabel('Accident Count')\n",
    "plt.title('Accidents by Maximum Speed Limit')\n",
    "file_path = os.path.join(city_folder_path, 'accidentsbymaxspeed_plot.png')\n",
    "plt.savefig(file_path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_average_lanes(lane):\n",
    "    if pd.isna(lane):\n",
    "        return np.nan\n",
    "    # Extract numeric values from the string\n",
    "    lane_values = re.findall(r'\\d+', str(lane))\n",
    "    # Convert extracted values to integers and calculate the mean\n",
    "    lane_values = [int(val) for val in lane_values]\n",
    "    return np.mean(lane_values) if lane_values else np.nan\n",
    "\n",
    "# Apply the cleaning and averaging function to the 'lanes' column\n",
    "accidents_near_edges['lanes'] = accidents_near_edges['lanes'].apply(clean_and_average_lanes)\n",
    "\n",
    "# Check the transformed 'lanes' column\n",
    "# print(accidents_near_edges['lanes'])\n",
    "\n",
    "def clean_and_maximize_maxspeed(speed):\n",
    "    if pd.isna(speed):\n",
    "        return np.nan\n",
    "    # Extract numeric values from the string\n",
    "    speed_values = re.findall(r'\\d+', str(speed))\n",
    "    # Convert extracted values to integers and use the maximum\n",
    "    speed_values = [int(val) for val in speed_values]\n",
    "    return max(speed_values) if speed_values else np.nan\n",
    "\n",
    "# Apply the cleaning and maximizing function to the 'maxspeed' column\n",
    "accidents_near_edges['maxspeed'] = accidents_near_edges['maxspeed'].apply(clean_and_maximize_maxspeed)\n",
    "\n",
    "# Check the transformed 'maxspeed' column\n",
    "#print(accidents_near_edges['maxspeed'])\n",
    "\n",
    "# Ensure 'lanes' and 'maxspeed' are numeric\n",
    "accidents_near_edges['lanes'] = pd.to_numeric(accidents_near_edges['lanes'], errors='coerce')\n",
    "accidents_near_edges['maxspeed'] = pd.to_numeric(accidents_near_edges['maxspeed'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# Histograms for visual inspection\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(accidents_near_edges['lanes'].dropna(), kde=True)\n",
    "plt.title('Distribution of Lanes')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(accidents_near_edges['maxspeed'].dropna(), kde=True)\n",
    "plt.title('Distribution of Max Speed')\n",
    "file_path = os.path.join(city_folder_path, 'lanes_speed_distribution_plot.png')\n",
    "plt.savefig(file_path)\n",
    "plt.show()\n",
    "\n",
    "# Shapiro-Wilk normality test\n",
    "shapiro_lanes = shapiro(accidents_near_edges['lanes'].dropna())\n",
    "shapiro_maxspeed = shapiro(accidents_near_edges['maxspeed'].dropna())\n",
    "\n",
    "print(f\"Shapiro test for lanes: {shapiro_lanes}\")\n",
    "print(f\"Shapiro test for maxspeed: {shapiro_maxspeed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import  spearmanr\n",
    "\n",
    "# Drop NaN values for correlation analysis\n",
    "accidents_for_correlation = accidents_near_edges.dropna(subset=['lanes', 'maxspeed'])\n",
    "\n",
    "# Pearson Correlation\n",
    "# pearson_corr, pearson_pval = pearsonr(accidents_for_correlation['lanes'], accidents_for_correlation['maxspeed'])\n",
    "# print(f\"Pearson Correlation: {pearson_corr}, P-value: {pearson_pval}\")\n",
    "\n",
    "# Spearman Correlation\n",
    "spearman_corr, spearman_pval = spearmanr(accidents_for_correlation['lanes'], accidents_for_correlation['maxspeed'])\n",
    "print(f\"Spearman Correlation: {spearman_corr}, P-value: {spearman_pval}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-hoc Analysis\n",
    "\n",
    "# 1) Subgroup Analysis\n",
    "\n",
    "road_types = accidents_near_edges['highway'].unique()\n",
    "\n",
    "for road_type in road_types:\n",
    "    subset = accidents_near_edges[accidents_near_edges['highway'] == road_type]\n",
    "    spearman_corr, spearman_pval = spearmanr(subset['lanes'], subset['maxspeed'], nan_policy='omit')\n",
    "    print(f\"Road Type: {road_type}, Spearman Correlation: {spearman_corr}, P-value: {spearman_pval}\")\n",
    "\n",
    "\n",
    "# Road types that had nan correlations\n",
    "road_types_nan_corr = ['cycleway', 'primary_link', 'tertiary_link', 'living_street', 'motorway']\n",
    "\n",
    "for road_type in road_types_nan_corr:\n",
    "    print(f\"Road Type: {road_type}\")\n",
    "    subset = accidents_near_edges[accidents_near_edges['highway'] == road_type]\n",
    "    \n",
    "    # Investigating 'lanes'\n",
    "    print(\"Lanes Distribution:\")\n",
    "    print(subset['lanes'].value_counts(dropna=False))\n",
    "\n",
    "    # Investigating 'maxspeed'\n",
    "    print(\"Maxspeed Distribution:\")\n",
    "    print(subset['maxspeed'].value_counts(dropna=False))\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Road types for the variance check\n",
    "road_types = accidents_near_edges['highway'].unique()\n",
    "\n",
    "for road_type in road_types:\n",
    "    subset = accidents_near_edges[accidents_near_edges['highway'] == road_type]\n",
    "\n",
    "    # Calculating variance\n",
    "    lanes_variance = subset['lanes'].var()\n",
    "    maxspeed_variance = subset['maxspeed'].var()\n",
    "\n",
    "    # Check if variance is close to zero\n",
    "    if lanes_variance < 0.01 or maxspeed_variance < 0.01:\n",
    "        print(f\"Skipping correlation test for road type {road_type} due to low variance in lanes or maxspeed.\")\n",
    "    else:\n",
    "        # Perform the correlation test\n",
    "        spearman_corr, spearman_pval = spearmanr(subset['lanes'], subset['maxspeed'], nan_policy='omit')\n",
    "        print(f\"Road Type: {road_type}, Spearman Correlation: {spearman_corr}, P-value: {spearman_pval}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For road types like 'unclassified', 'secondary', 'trunk' and others, there are significant Spearman correlation coefficients, suggesting a meaningful statistical relationship between the number of lanes and max speed on roads with accidents. \n",
    "\n",
    "For road types like 'cycleway', 'primary_link', 'tertiary_link', 'living_street', and 'motorway', either the correlation results are nan, or the test was skipped due to low variance. This typically indicates that there isn't enough variability in either the number of lanes or max speed data for those road types in your dataset, or that all values are constant. \n",
    "For example: in 'cycleway', both lanes and max speed are NaN for all entries, leading to undefined correlation, in 'primary_link', all max speed values are 50, providing no variability.\n",
    "Similar situations are observed for 'tertiary_link', 'living_street', and 'motorway'.\n",
    "\n",
    "**Interpretation and Next Steps**\n",
    "\n",
    "The findings here suggest a relationship between the number of lanes and the maximum speed limit that varies by road type. For example, the negative correlation on 'secondary' roads might indicate complex traffic conditions where additional lanes do not necessarily lead to higher speed limits, potentially due to factors like traffic density or road design. The lack of variability in these categories means that correlation analysis isn't informative for these road types. This could be due to the nature of these roads (e.g., 'cycleways' typically don't vary in the number of lanes or speed limits) or data limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accident Frequency Near Roads with Different LTS Values\n",
    "\n",
    "accidents_filtered = accidents_near_edges[(accidents_near_edges['lts'] != 0) & (accidents_near_edges['lts'].notna())].copy()\n",
    "accidents_by_lts = accidents_filtered.groupby('lts').size().reset_index(name='accident_count')\n",
    "accidents_by_lts.sort_values('accident_count', ascending=False, inplace=True)\n",
    "total_accidents = accidents_by_lts['accident_count'].sum()\n",
    "accidents_by_lts['percentage'] = (accidents_by_lts['accident_count'] / total_accidents) * 100\n",
    "\n",
    "# Plotting LTS values\n",
    "custom_palette = [\"forestgreen\", \"dodgerblue\", \"#f4e800\", \"firebrick\"]  \n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='lts', y='accident_count', data=accidents_by_lts, palette=custom_palette)\n",
    "plt.title('Number of Accidents by LTS Value')\n",
    "plt.xlabel('LTS Value')\n",
    "plt.ylabel('Number of Accidents')\n",
    "file_path = os.path.join(city_folder_path, 'accidents_lts_plot.png')\n",
    "plt.savefig(file_path)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='lts', y='percentage', data=accidents_by_lts, palette=custom_palette)\n",
    "plt.title('Percentage of Accidents by LTS Value')\n",
    "plt.xlabel('LTS Value')\n",
    "plt.ylabel('Percentage of Accidents')\n",
    "file_path = os.path.join(city_folder_path, 'perc_accidents_lts_plot.png')\n",
    "plt.savefig(file_path)\n",
    "plt.show()\n",
    "\n",
    "# Creating stress level categories\n",
    "accidents_filtered['stress_level'] = accidents_filtered['lts'].apply(lambda x: 'low stress' if x in [1, 2] else ('high stress' if x in [3, 4] else 'undefined'))\n",
    "accidents_stress_level = accidents_filtered[accidents_filtered['stress_level'] != 'undefined']\n",
    "accidents_by_stress_level = accidents_stress_level.groupby('stress_level').size().reset_index(name='accident_count')\n",
    "total_accidents_stress_level = accidents_by_stress_level['accident_count'].sum()\n",
    "accidents_by_stress_level['percentage'] = (accidents_by_stress_level['accident_count'] / total_accidents_stress_level) * 100\n",
    "\n",
    "custom_palette_s = ['firebrick', 'forestgreen']  \n",
    "\n",
    "# Plotting Stress Levels\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='stress_level', y='accident_count', data=accidents_by_stress_level, palette=custom_palette_s)\n",
    "plt.title('Number of Accidents by Stress Level')\n",
    "plt.xlabel('Stress Level')\n",
    "plt.ylabel('Number of Accidents')\n",
    "file_path = os.path.join(city_folder_path, 'accidents_stress_level_plot.png')\n",
    "plt.savefig(file_path)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='stress_level', y='percentage', data=accidents_by_stress_level, palette=custom_palette_s)\n",
    "plt.title('Percentage of Accidents by Stress Level')\n",
    "plt.xlabel('Stress Level')\n",
    "plt.ylabel('Percentage of Accidents')\n",
    "file_path = os.path.join(city_folder_path, 'perc_accidents_stress_level_plot.png')\n",
    "plt.savefig(file_path)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Nodes\n",
    "\n",
    "accidents_filtered_n = accidents_near_nodes[(accidents_near_nodes['lts'] != 0) & (accidents_near_nodes['lts'].notna())].copy()\n",
    "accidents_by_lts_n = accidents_filtered_n.groupby('lts').size().reset_index(name='accident_count')\n",
    "accidents_by_lts_n.sort_values('accident_count', ascending=False, inplace=True)\n",
    "total_accidents_n = accidents_by_lts_n['accident_count'].sum()\n",
    "accidents_by_lts_n['percentage'] = (accidents_by_lts_n['accident_count'] / total_accidents_n) * 100\n",
    "\n",
    "# Plotting LTS values\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='lts', y='accident_count', data=accidents_by_lts_n, palette=custom_palette)\n",
    "plt.title('Number of Accidents by LTS Value - Intersections')\n",
    "plt.xlabel('LTS Value')\n",
    "plt.ylabel('Number of Accidents')\n",
    "file_path = os.path.join(city_folder_path, 'accidents_lts_intersection_plot.png')\n",
    "plt.savefig(file_path)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='lts', y='percentage', data=accidents_by_lts_n, palette=custom_palette)\n",
    "plt.title('Percentage of Accidents by LTS Value - Intersections')\n",
    "plt.xlabel('LTS Value')\n",
    "plt.ylabel('Percentage of Accidents')\n",
    "file_path = os.path.join(city_folder_path, 'perc_accidents_lts_intersection_plot.png')\n",
    "plt.savefig(file_path)\n",
    "plt.show()\n",
    "\n",
    "# Creating stress level categories\n",
    "accidents_filtered_n['stress_level'] = accidents_filtered_n['lts'].apply(lambda x: 'low stress' if x in [1, 2] else ('high stress' if x in [3, 4] else 'undefined'))\n",
    "accidents_stress_level_n = accidents_filtered_n[accidents_filtered_n['stress_level'] != 'undefined']\n",
    "accidents_by_stress_level_n = accidents_stress_level_n.groupby('stress_level').size().reset_index(name='accident_count')\n",
    "total_accidents_stress_level_n = accidents_by_stress_level_n['accident_count'].sum()\n",
    "accidents_by_stress_level_n['percentage'] = (accidents_by_stress_level_n['accident_count'] / total_accidents_stress_level_n) * 100\n",
    "\n",
    "# Plotting Stress Levels\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='stress_level', y='accident_count', data=accidents_by_stress_level_n, palette=custom_palette_s)\n",
    "plt.title('Number of Accidents by Stress Level - Intersections')\n",
    "plt.xlabel('Stress Level')\n",
    "plt.ylabel('Number of Accidents')\n",
    "file_path = os.path.join(city_folder_path, 'accidents_stress_level_intersection_plot.png')\n",
    "plt.savefig(file_path)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='stress_level', y='percentage', data=accidents_by_stress_level_n, palette=custom_palette_s)\n",
    "plt.title('Percentage of Accidents by Stress Level - Intersections')\n",
    "plt.xlabel('Stress Level')\n",
    "plt.ylabel('Percentage of Accidents')\n",
    "file_path = os.path.join(city_folder_path, 'perc_accidents_stress_level_intersection_plot.png')\n",
    "plt.savefig(file_path)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if the difference is significant or not in both scenarios\n",
    "\n",
    "#Goodness-of-Fit Chi-Square test => test compares the observed counts to expected counts under a null hypothesis that all categories have the same frequency.\n",
    "contingency_table = accidents_by_lts.pivot_table(index='lts', values='accident_count', aggfunc='sum')\n",
    "\n",
    "from scipy.stats import chisquare\n",
    "\n",
    "# Using the Goodness-of-Fit Chi-Square test\n",
    "chi2, p = chisquare(contingency_table['accident_count'])\n",
    "\n",
    "# Interpreting the results\n",
    "print(\"Chi-Square value:\", chi2)\n",
    "print(f\"P-value: {p:.10f}\")\n",
    "\n",
    "if p < 0.05:\n",
    "    # Pairwise comparisons (example with a simple loop, consider Bonferroni correction)\n",
    "    for i in range(len(contingency_table)):\n",
    "        for j in range(i+1, len(contingency_table)):\n",
    "            # Extract only the accident count values for the comparison\n",
    "            obs = [contingency_table['accident_count'].iloc[i], contingency_table['accident_count'].iloc[j]]\n",
    "            chi2, p_pairwise = chisquare(obs)\n",
    "            if p_pairwise < 0.05:  # Adjust this threshold for multiple tests\n",
    "                print(f\"Significant difference between LTS {contingency_table.index[i]} and LTS {contingency_table.index[j]} (p={p_pairwise:.10f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if the difference is significant or not in both scenarios - intersections\n",
    "\n",
    "#Goodness-of-Fit Chi-Square test => test compares the observed counts to expected counts under a null hypothesis that all categories have the same frequency.\n",
    "contingency_table = accidents_by_lts_n.pivot_table(index='lts', values='accident_count', aggfunc='sum')\n",
    "\n",
    "from scipy.stats import chisquare\n",
    "\n",
    "# Using the Goodness-of-Fit Chi-Square test\n",
    "chi2, p = chisquare(contingency_table['accident_count'])\n",
    "\n",
    "# Interpreting the results\n",
    "print(\"Chi-Square value:\", chi2)\n",
    "print(f\"P-value: {p:.10f}\")\n",
    "\n",
    "if p < 0.05:\n",
    "    # Pairwise comparisons (example with a simple loop, consider Bonferroni correction)\n",
    "    for i in range(len(contingency_table)):\n",
    "        for j in range(i+1, len(contingency_table)):\n",
    "            # Extract only the accident count values for the comparison\n",
    "            obs = [contingency_table['accident_count'].iloc[i], contingency_table['accident_count'].iloc[j]]\n",
    "            chi2, p_pairwise = chisquare(obs)\n",
    "            if p_pairwise < 0.05:  # Adjust this threshold for multiple tests\n",
    "                print(f\"Significant difference between LTS {contingency_table.index[i]} and LTS {contingency_table.index[j]} (p={p_pairwise:.10f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing a contingency table for the Chi-Square test\n",
    "contingency_table = accidents_by_stress_level.pivot_table(index='stress_level', values='accident_count', aggfunc='sum')\n",
    "\n",
    "# Using the Goodness-of-Fit Chi-Square test\n",
    "chi2, p = chisquare(contingency_table['accident_count'])\n",
    "\n",
    "# Interpreting the results\n",
    "print(\"Chi-Square value:\", chi2)\n",
    "print(f\"P-value: {p:.10f}\")\n",
    "\n",
    "if p < 0.05:\n",
    "    # Pairwise comparisons (example with a simple loop, consider Bonferroni correction)\n",
    "    for i in range(len(contingency_table)):\n",
    "        for j in range(i+1, len(contingency_table)):\n",
    "            # Extract only the accident count values for the comparison\n",
    "            obs = [contingency_table['accident_count'].iloc[i], contingency_table['accident_count'].iloc[j]]\n",
    "            chi2, p_pairwise = chisquare(obs)\n",
    "            if p_pairwise < 0.05:  # Adjust this threshold for multiple tests\n",
    "                print(f\"Significant difference between LTS {contingency_table.index[i]} and LTS {contingency_table.index[j]} (p={p_pairwise:.10f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing a contingency table for the Chi-Square test\n",
    "contingency_table = accidents_by_stress_level_n.pivot_table(index='stress_level', values='accident_count', aggfunc='sum')\n",
    "\n",
    "# Using the Goodness-of-Fit Chi-Square test\n",
    "chi2, p = chisquare(contingency_table['accident_count'])\n",
    "\n",
    "# Interpreting the results\n",
    "print(\"Chi-Square value:\", chi2)\n",
    "print(f\"P-value: {p:.10f}\")\n",
    "\n",
    "if p < 0.05:\n",
    "    # Pairwise comparisons (example with a simple loop, consider Bonferroni correction)\n",
    "    for i in range(len(contingency_table)):\n",
    "        for j in range(i+1, len(contingency_table)):\n",
    "            # Extract only the accident count values for the comparison\n",
    "            obs = [contingency_table['accident_count'].iloc[i], contingency_table['accident_count'].iloc[j]]\n",
    "            chi2, p_pairwise = chisquare(obs)\n",
    "            if p_pairwise < 0.05:  # Adjust this threshold for multiple tests\n",
    "                print(f\"Significant difference between LTS {contingency_table.index[i]} and LTS {contingency_table.index[j]} (p={p_pairwise:.10f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "coordinates = accidents_near_edges[['x_gps', 'y_gps']]\n",
    "\n",
    "# DBSCAN Clustering\n",
    "dbscan = DBSCAN(eps=0.007, min_samples=55)  \n",
    "dbscan_clusters = dbscan.fit_predict(coordinates)\n",
    "accidents_near_edges['dbscan_cluster'] = dbscan_clusters\n",
    "\n",
    "# Filter out noise points (DBSCAN labels noise as -1)\n",
    "clustered_data = coordinates[dbscan_clusters != -1]\n",
    "clustered_labels = dbscan_clusters[dbscan_clusters != -1]\n",
    "\n",
    "# Compute Silhouette Score\n",
    "if len(set(clustered_labels)) > 1:  # Need at least 2 clusters to compute silhouette score\n",
    "    silhouette_avg = silhouette_score(clustered_data, clustered_labels)\n",
    "    print(\"Silhouette Score: \", silhouette_avg)\n",
    "else:\n",
    "    print(\"Not enough clusters to compute Silhouette Score\")\n",
    "\n",
    "# Compute Calinski-Harabasz Index\n",
    "calinski_harabasz = calinski_harabasz_score(clustered_data, clustered_labels)\n",
    "print(\"Calinski-Harabasz Index: \", calinski_harabasz)\n",
    "\n",
    "# Compute Davies-Bouldin Index\n",
    "davies_bouldin = davies_bouldin_score(clustered_data, clustered_labels)\n",
    "print(\"Davies-Bouldin Index: \", davies_bouldin)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(accidents_near_edges['x_gps'], accidents_near_edges['y_gps'], c=accidents_near_edges['dbscan_cluster'], cmap='viridis')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('DBSCAN Accident Clusters')\n",
    "file_path = os.path.join(city_folder_path, 'DBSCAN_accident_clusters_plot.png')\n",
    "plt.savefig(file_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the number of accidents in each cluster\n",
    "cluster_counts = accidents_near_edges['dbscan_cluster'].value_counts().sort_index()\n",
    "print(\"Cluster Counts:\\n\", cluster_counts)\n",
    "\n",
    "# Determine the number of clusters (excluding outliers marked as -1)\n",
    "num_clusters = len(cluster_counts) - (1 if -1 in cluster_counts else 0)\n",
    "print(\"\\nNumber of Clusters (excluding outliers):\", num_clusters)\n",
    "\n",
    "# Convert to a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(accidents_near_edges, geometry=gpd.points_from_xy(accidents_near_edges.x_gps, accidents_near_edges.y_gps))\n",
    "\n",
    "# Calculate centroids of each cluster\n",
    "for cluster_num in range(num_clusters):\n",
    "    cluster_points = gdf[gdf['dbscan_cluster'] == cluster_num]\n",
    "    centroid = cluster_points.geometry.unary_union.centroid\n",
    "    print(f\"Centroid of Cluster {cluster_num}: {centroid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the lts folium level map from other file\n",
    "# add over this the incidents of the last 10 years in the first map\n",
    "# add over this the incidents of the last 5 years in the first map\n",
    "import folium\n",
    "import h3\n",
    "import os\n",
    "from shapely import wkt\n",
    "from shapely.geometry import box, Polygon\n",
    "from folium.plugins import Geocoder\n",
    "from folium import FeatureGroup, LayerControl\n",
    "\n",
    "color_palette = [\"forestgreen\", \"dodgerblue\", \"#f4e800\", \"firebrick\", \"#000000\", \"purple\"]\n",
    "lts_classes = [1, 2, 3, 4, \"undefined\", \"not cyclable\"]\n",
    "colors = dict(zip(lts_classes, color_palette))\n",
    "\n",
    "mean_latitude = all_lts_projected.geometry.apply(lambda geom: geom.centroid.y).mean()\n",
    "mean_longitude = all_lts_projected.geometry.apply(lambda geom: geom.centroid.x).mean()\n",
    "\n",
    "map_osm = folium.Map(location=[mean_latitude, mean_longitude], zoom_start=11.5)\n",
    "\n",
    "geocoder = Geocoder(\n",
    "    position='topleft',\n",
    "    control=True,\n",
    "    collapsed=True,\n",
    "    auto_type=True,\n",
    "    placeholder='Search for a location...',\n",
    "    max_zoom=15,\n",
    "    hide_marker=True\n",
    ").add_to(map_osm)\n",
    "\n",
    "feature_groups = {}\n",
    "\n",
    "for lts_class, color in colors.items():\n",
    "    if isinstance(lts_class, str):  # For \"Undefined\" and \"Not cyclable\"\n",
    "        name = lts_class\n",
    "    else:\n",
    "        name = f\"LTS {lts_class}\"\n",
    "    toggle_switch_html = f'''\n",
    "    <div class=\"toggle-switch active\" onclick=\"toggleSwitch(this)\" style=\"background-color: {color};\"></div>\n",
    "    <span>{name}</span>\n",
    "    '''\n",
    "    is_visible = False if lts_class == \"undefined\" else True\n",
    "    feature_group = FeatureGroup(name=toggle_switch_html, show=is_visible)\n",
    "    feature_groups[lts_class] = feature_group\n",
    "    feature_group.add_to(map_osm)\n",
    "\n",
    "# Function to generate hexagons\n",
    "def get_hexagons_for_bounds(bounds, resolution):\n",
    "    bounding_polygon = box(*bounds)\n",
    "    center = bounding_polygon.centroid\n",
    "    hex_center = h3.geo_to_h3(center.y, center.x, resolution)\n",
    "    hexagons = h3.k_ring(hex_center, 100)\n",
    "    hexagons = [hexagon for hexagon in hexagons if h3.h3_to_geo_boundary(hexagon, geo_json=True)]\n",
    "    return hexagons\n",
    "\n",
    "hexagons = get_hexagons_for_bounds(all_lts_projected.total_bounds, 9)\n",
    "bounding_polygon = box(*all_lts_projected.total_bounds)\n",
    "\n",
    "# Filter hexagons to keep only those intersecting with the bounding polygon\n",
    "hexagons = [h for h in hexagons if Polygon(h3.h3_to_geo_boundary(h, geo_json=True)).intersects(bounding_polygon)]\n",
    "\n",
    "hex_gdf = gpd.GeoDataFrame({'hex_id': hexagons, \n",
    "                            'geometry': [Polygon(h3.h3_to_geo_boundary(h, geo_json=True)) for h in hexagons]}, \n",
    "                           crs=\"EPSG:4326\")\n",
    "\n",
    "# Function to find prevalent LTS within each hexagon\n",
    "def get_prevalent_lts(hex_geometry):\n",
    "    # Find streets within the hexagon\n",
    "    streets_within_hex = all_lts_projected[all_lts_projected.intersects(hex_geometry)]\n",
    "    if streets_within_hex.empty:\n",
    "        return np.nan\n",
    "    # Return prevalent LTS\n",
    "    return streets_within_hex['lts'].value_counts().idxmax()\n",
    "\n",
    "hex_gdf['lts'] = hex_gdf['geometry'].apply(get_prevalent_lts)\n",
    "\n",
    "for _, row in hex_gdf.iterrows():\n",
    "    if pd.isna(row['lts']):\n",
    "        lts_class = \"undefined\"\n",
    "        color = colors[lts_class]\n",
    "    elif row['lts'] not in feature_groups:\n",
    "        lts_class = \"undefined\"\n",
    "        color = colors[lts_class]\n",
    "    else:\n",
    "        streets_within_hex = all_lts_projected[all_lts_projected.intersects(row['geometry'])]\n",
    "        names = streets_within_hex['name'].fillna(\"\").str.lower()\n",
    "        if any(names.str.contains(keyword).any() for keyword in [\"tangenziale\", \"superstrada\", \"strada statale\", \"autostrada\", \"strada a scorrimento veloce\"]) and not names.str.contains(\"bici|cicla|ciclo\").any():\n",
    "            lts_class = \"not cyclable\"\n",
    "            color = colors[lts_class]\n",
    "        else:\n",
    "            lts_class = row['lts']\n",
    "            color = colors[lts_class]\n",
    "    \n",
    "    folium.GeoJson(\n",
    "        row['geometry'], \n",
    "        style_function=lambda _, color=color: {'color': \"#000000\", 'fillColor': color, 'fillOpacity': 0.7, 'weight': 2}\n",
    "    ).add_to(feature_groups[lts_class])\n",
    "\n",
    "LayerControl(position='topright').add_to(map_osm)\n",
    "\n",
    "def add_accidents_to_map(accidents, map_object):\n",
    "    for _, row in accidents.iterrows():\n",
    "        folium.CircleMarker(\n",
    "            location=[row.geometry.y, row.geometry.x],\n",
    "            radius=2,\n",
    "            color='red',\n",
    "            fill=True,\n",
    "            fill_color='red',\n",
    "            fill_opacity=0.7\n",
    "        ).add_to(map_object)\n",
    "\n",
    "# def add_accidents_to_map_5(accidents, map_object):\n",
    "#     for _, row in accidents.iterrows():\n",
    "#         folium.CircleMarker(\n",
    "#             location=[row.geometry.y, row.geometry.x],\n",
    "#             radius=2,\n",
    "#             color='red',\n",
    "#             fill=True,\n",
    "#             fill_color='darkred',\n",
    "#             fill_opacity=0.7\n",
    "#         ).add_to(map_object)\n",
    "\n",
    "# Create map with accidents from the last 10 years\n",
    "# add_accidents_to_map(accidents_last_10_years, map_osm_10_years)\n",
    "add_accidents_to_map(accidents_gdf, map_osm)\n",
    "\n",
    "map_html = open(\"custom_map.html\", \"r\").read()\n",
    "map_osm.get_root().html.add_child(folium.Element(map_html))\n",
    "\n",
    "file_path = os.path.join(city_folder_path, 'choropleth_lts_accidents_map.html')\n",
    "\n",
    "# Assuming 'accident_map' is a Folium Map object\n",
    "map_osm.save(file_path)\n",
    "\n",
    "map_osm\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
