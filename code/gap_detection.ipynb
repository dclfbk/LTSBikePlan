{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of disconnected components on mnw: 328\n",
      "size of lcc: 901\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import math\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import igraph as ig\n",
    "from ast import literal_eval\n",
    "import pickle as pkl\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Useful functions\n",
    "\n",
    "def make_attr_dict(*args, **kwargs): \n",
    "    \n",
    "    argCount = len(kwargs)\n",
    "    \n",
    "    if argCount > 0:\n",
    "        attributes = {}\n",
    "        for kwarg in kwargs:\n",
    "            attributes[kwarg] = kwargs.get(kwarg, None)\n",
    "        return attributes\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "# Load data\n",
    "nodes_path = \"../data/Montereale_Valcellina_gdf_nodes.csv\"\n",
    "edges_path = \"../data/Montereale_Valcellina_all_lts.csv\"\n",
    "nodes_data = pd.read_csv(nodes_path)\n",
    "edges_data = pd.read_csv(edges_path)\n",
    "\n",
    "def create_node_attr_dict(row):\n",
    "    return make_attr_dict(coord=row.geometry, lts=int(row.lts) if not pd.isna(row.lts) else -1)\n",
    "\n",
    "def create_edge_attr_dict(row):\n",
    "    return make_attr_dict(length=row.length, edge_id=row.edge_id, coord=row.geometry, lts=row.lts, intnodes=[])\n",
    "\n",
    "def create_edge_id(row):\n",
    "    return str(sorted([row[\"u\"], row[\"v\"]]))\n",
    "\n",
    "def get_min_node(row):\n",
    "    return min(row[\"u\"], row[\"v\"])\n",
    "\n",
    "def get_max_node(row):\n",
    "    return max(row[\"u\"], row[\"v\"])\n",
    "\n",
    "def convert_to_string(data):\n",
    "    return str(data) if isinstance(data, list) else data\n",
    "\n",
    "def convert_attributes(graph):\n",
    "    for _, attributes in graph.nodes(data=True):\n",
    "        for key, value in attributes.items():\n",
    "            attributes[key] = convert_to_string(value)\n",
    "\n",
    "    for _, _, attributes in graph.edges(data=True):\n",
    "        for key, value in attributes.items():\n",
    "            attributes[key] = convert_to_string(value)\n",
    "\n",
    "# Create node and edge attribute dictionaries\n",
    "nodes_data[\"attr_dict\"] = nodes_data.apply(create_node_attr_dict, axis=1)\n",
    "edges_data[\"edge_id\"] = edges_data.apply(create_edge_id, axis=1)\n",
    "\n",
    "# Drop duplicates\n",
    "edges_data = edges_data.drop_duplicates(subset=[\"osmid\", \"oneway\", \"edge_id\", \"length\"], keep=\"first\").reset_index(drop=True)\n",
    "edges_data[\"attr_dict\"] = edges_data.apply(create_edge_attr_dict, axis=1)\n",
    "\n",
    "# Sort and rename columns\n",
    "edges_data[\"orig\"] = edges_data.apply(get_min_node, axis=1)\n",
    "edges_data[\"dest\"] = edges_data.apply(get_max_node, axis=1)\n",
    "edges_data = edges_data.drop(columns=[\"u\", \"v\"])\n",
    "\n",
    "# Create graph\n",
    "mnw = nx.Graph()\n",
    "mnw.add_nodes_from(nodes_data[[\"osmid\", \"attr_dict\"]].itertuples(index=False))\n",
    "mnw.add_edges_from(edges_data[[\"orig\", \"dest\", \"attr_dict\"]].itertuples(index=False))\n",
    "\n",
    "# Convert attributes to string\n",
    "convert_attributes(mnw)\n",
    "\n",
    "# Save the graph\n",
    "nx.write_graphml(mnw, \"../data/mnw.graphml\")\n",
    "\n",
    "# Get connected components\n",
    "cd_nodeset = list(nx.connected_components(mnw))\n",
    "print(f\"number of disconnected components on mnw: {len(cd_nodeset)}\")\n",
    "\n",
    "# Extract component details\n",
    "cd_size = [len(comp) for comp in cd_nodeset]\n",
    "cd_network = [nx.subgraph(mnw, comp) for comp in cd_nodeset]\n",
    "cd_coord_dict = [nx.get_edge_attributes(net, \"coord\") for net in cd_network]\n",
    "cd_coord_list = [[coords[key] for key in coords.keys()] for coords in cd_coord_dict]\n",
    "\n",
    "# Create dataframe with component details\n",
    "comps = pd.DataFrame({\n",
    "    'nodeset': cd_nodeset, \n",
    "    'size': cd_size,\n",
    "    'network': cd_network,\n",
    "    'coord': cd_coord_list\n",
    "})\n",
    "\n",
    "# Get the largest connected component\n",
    "lcc = comps[\"size\"].max()\n",
    "print(f\"size of lcc: {lcc}\")\n",
    "\n",
    "comps = comps.sort_values(by=\"size\", ascending=False).reset_index(drop=True)\n",
    "mnwl_nodes = comps.iloc[0][\"nodeset\"]\n",
    "mnwl_edges = edges_data[edges_data[\"orig\"].isin(mnwl_nodes)].copy().reset_index(drop=True)\n",
    "mnwl = nx.subgraph(mnw, mnwl_nodes)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1\n",
      "Run 2\n",
      "Run 3\n",
      "Run 4\n",
      "Run 5\n",
      "Run 6\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "def get_sorted_neighbors(graph, node):\n",
    "    return sorted([n for n in graph.neighbors(node)])\n",
    "\n",
    "def simplify_network(mnwl):\n",
    "    H = mnwl.copy()\n",
    "    simplify_further = True\n",
    "    run = 0\n",
    "\n",
    "    while simplify_further:\n",
    "        run += 1\n",
    "        print(f\"Run {run}\")\n",
    "\n",
    "        points_all_list = sorted(list(H.nodes))\n",
    "        degrees_all_list = [H.degree(node) for node in points_all_list]\n",
    "\n",
    "        pointsall = pd.DataFrame({\n",
    "            \"osmid\": points_all_list,\n",
    "            \"d\": degrees_all_list,\n",
    "            \"remove\": None\n",
    "        })\n",
    "\n",
    "        pointsend = pointsall[pointsall[\"d\"] != 2].copy()\n",
    "        pointsend[\"remove\"] = False\n",
    "\n",
    "        pointsd2 = pointsall[pointsall[\"d\"] == 2].copy()\n",
    "        pointsd2[\"remove\"] = True\n",
    "\n",
    "        nodes_interstitial = pointsd2[pointsd2[\"remove\"] == True].copy()\n",
    "\n",
    "        eint = nodes_interstitial.copy()\n",
    "        eint[\"orig\"] = eint[\"osmid\"].apply(lambda x: get_sorted_neighbors(H, x)[0])\n",
    "        eint[\"dest\"] = eint[\"osmid\"].apply(lambda x: get_sorted_neighbors(H, x)[1])\n",
    "\n",
    "        lendict = nx.get_edge_attributes(H, \"length\")\n",
    "        eint[\"length_new\"] = eint.apply(lambda x: np.sum([lendict[tuple(sorted(edge))] for edge in H.edges(x.osmid)]), axis=1)\n",
    "\n",
    "        stack = list(np.unique(eint[\"osmid\"]))\n",
    "\n",
    "        Hprior = H.copy()\n",
    "\n",
    "        intnodesdict = nx.get_edge_attributes(H, \"intnodes\")\n",
    "        edgecoorddict = nx.get_edge_attributes(H, \"coord\")\n",
    "\n",
    "        nodes_removed = 0\n",
    "\n",
    "        while stack:\n",
    "            mynode = stack.pop()\n",
    "\n",
    "            for n in nx.neighbors(Hprior, mynode):\n",
    "                if n in stack:\n",
    "                    stack.remove(n)\n",
    "\n",
    "            u, v = eint.loc[eint[\"osmid\"] == mynode, [\"orig\", \"dest\"]].values[0]\n",
    "\n",
    "            if (u, v) not in H.edges:\n",
    "                myintnodes = [intnodesdict[tuple(sorted(edge))] for edge in H.edges(mynode)]\n",
    "                myintnodes.append([mynode])\n",
    "\n",
    "                H.add_edge(u, v,\n",
    "                           length=eint.loc[eint[\"osmid\"] == mynode][\"length_new\"].values[0],\n",
    "                           intnodes=myintnodes,\n",
    "                           edge_id=str(sorted([u, v])),\n",
    "                           coord=edgecoorddict.get(tuple(sorted([u, mynode])), []) + edgecoorddict.get(tuple(sorted([v, mynode])), []))\n",
    "\n",
    "                H.remove_node(mynode)\n",
    "                nodes_removed += 1\n",
    "\n",
    "        if nodes_removed == 0:\n",
    "            simplify_further = False\n",
    "            print(\"Done\")\n",
    "\n",
    "    return H\n",
    "\n",
    "# Simplify the network and save\n",
    "H = simplify_network(mnwl)\n",
    "nx.write_gpickle(H, \"../data/H.gpickle\")\n",
    "\n",
    "# conversion to igraph\n",
    "h = ig.Graph.from_networkx(H)\n",
    "# to read in again: Graph.Read_Pickle()\n",
    "\n",
    "# eids: \"conversion table\" for edge ids from igraph to nx \n",
    "eids_nx = [tuple(sorted(literal_eval(h.es(i)[\"edge_id\"][0]))) for i in range(len(h.es))]\n",
    "eids_ig = [i for i in range(len(h.es))]\n",
    "eids_conv = pd.DataFrame({\"nx\": eids_nx, \"ig\": eids_ig})\n",
    "\n",
    "# nids: \"conversion table\" for node ids from igraph to nx\n",
    "nids_nx = [h.vs(i)[\"_nx_name\"][0] for i in range(len(h.vs))]\n",
    "nids_ig = [i for i in range(len(h.vs))]\n",
    "nids_conv = pd.DataFrame({\"nx\": nids_nx, \"ig\": nids_ig})\n",
    "\n",
    "eids_conv.to_pickle(\"../data/eids_conv.pickle\")\n",
    "nids_conv.to_pickle(\"../data/nids_conv.pickle\")\n",
    "\n",
    "#eids_conv_read = pd.read_pickle(\"../data/eids_conv.pickle\")\n",
    "#print(eids_conv_read.head())\n",
    "\n",
    "#nids_conv_read = pd.read_pickle(\"../data/nids_conv.pickle\")\n",
    "#print(nids_conv_read.head())\n",
    "\n",
    "# extract edge and node attributes as dictionaries\n",
    "\n",
    "# make data frame of ebc with:\n",
    "ebc = pd.DataFrame({\"edge_ig\": [e.index for e in h.es]}) # igraph edge ID\n",
    "ebc[\"edge_nx\"] = ebc.apply(lambda x: tuple(literal_eval(h.es[x.edge_ig][\"edge_id\"])), axis = 1) # nx edge ID\n",
    "ebc[\"length\"] = ebc.apply(lambda x: h.es[x.edge_ig][\"length\"], axis = 1) # length in meters\n",
    "\n",
    "# compute ebcs:\n",
    "ebc[\"ebc_inf\"] = h.edge_betweenness(directed = False, cutoff = None, weights = \"length\") # \"standard\" ebc\n",
    "ebc[\"ebc_lambda\"] = h.edge_betweenness(directed = False, cutoff = 2500, weights = \"length\") # ebc only including *paths* below 2500m\n",
    "ebc.to_pickle(\"../data/ebc.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196251  gaps found\n"
     ]
    }
   ],
   "source": [
    "# Step 1-2: Identify and prioritize gaps\n",
    "ced = nx.get_edge_attributes(H, \"coord\")\n",
    "\n",
    "# Useful functions\n",
    "\n",
    "# computes pathlength by nx - handling error message if nodes are not connected/not part of the network\n",
    "def pathlength_if_connected(my_nw, my_o, my_d):\n",
    "    try:\n",
    "        return nx.dijkstra_path_length(my_nw, my_o, my_d, weight=\"length\")\n",
    "    except nx.NetworkXNoPath:\n",
    "        return math.inf\n",
    "    \n",
    "def extract_coords_from_linestring(linestring):\n",
    "    # Extract coordinates using regex\n",
    "    matches = re.findall(r'(\\d+\\.\\d+) (\\d+\\.\\d+)', linestring)\n",
    "    return [(float(y), float(x)) for x, y in matches]\n",
    "\n",
    "def get_path_coords(my_path, my_coorddict):\n",
    "    pathcoords = []\n",
    "    for edge_id in my_path:\n",
    "        linestring = my_coorddict[tuple(sorted(edge_id))]\n",
    "        edge_coords = extract_coords_from_linestring(linestring)\n",
    "        pathcoords.append(edge_coords)\n",
    "    return pathcoords\n",
    "\n",
    "# Identify all the gaps:\n",
    "\n",
    "# shortest_path_list = list of shortest paths for all possible contact-to-contact node combinations\n",
    "\n",
    "shortest_path_list = []\n",
    "\n",
    "if not os.path.exists(\"../data/chunks\"):\n",
    "    os.mkdir(\"../data/chunks\")\n",
    "\n",
    "# ALL CONTACT NODES FROM THE NETWORK\n",
    "nodestack = [node.index for node in h.vs()]\n",
    "\n",
    "count = 0\n",
    "\n",
    "while nodestack:\n",
    "    \n",
    "    node = nodestack.pop()\n",
    "    \n",
    "    shortest_path_list = shortest_path_list + h.get_shortest_paths(node, to=nodestack, weights=\"length\", mode=\"out\", output = \"epath\")\n",
    "    \n",
    "    if len(shortest_path_list) >= 2*10**5:\n",
    "        with open(\"../data/chunks/c\" + str(count) + \".pickle\", 'wb') as handle:\n",
    "            pkl.dump(shortest_path_list, handle, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "        del(shortest_path_list)\n",
    "        count += 1\n",
    "        shortest_path_list = []\n",
    "        \n",
    "        \n",
    "# Saving the last chunck (WITH LEN < 2*10**5)\n",
    "with open(\"../data/chunks/c\" + str(count) + \".pickle\", 'wb') as handle:\n",
    "    pkl.dump(shortest_path_list, handle, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "\n",
    "del(shortest_path_list)\n",
    "\n",
    "cs = set()\n",
    "for edge in eids_conv[\"ig\"]:\n",
    "        cs.add(edge)\n",
    "\n",
    "mygaps = []\n",
    "\n",
    "# Chunkwise:\n",
    "\n",
    "mychunks = [\"../data/chunks/\" + filename for filename in os.listdir(\"../data/chunks/\")]\n",
    "\n",
    "for chunk in mychunks:\n",
    "    \n",
    "    with open(chunk, 'rb') as f:\n",
    "        pathlist = pkl.load(f)\n",
    "\n",
    "    # adding the item to the gaplist only if it consists of only-car-edges\n",
    "    gaplist = [item for item in pathlist if set(item).issubset(cs)]\n",
    "\n",
    "    mygaps = mygaps + gaplist\n",
    "    \n",
    "    del(gaplist, pathlist)\n",
    "    \n",
    "print(len(mygaps), \" gaps found\")\n",
    "\n",
    "# remove chunks (not needed anymore)\n",
    "for chunk in mychunks:\n",
    "    os.remove(chunk)\n",
    "os.rmdir(\"../data/chunks\")\n",
    "\n",
    "# Convert the shortest_path list to a dataframe and add length, origin and destination\n",
    "\n",
    "# to df\n",
    "mygaps = pd.DataFrame({\"path\": mygaps})\n",
    "\n",
    "# add length\n",
    "mygaps[\"length\"] = mygaps.apply(lambda x: np.sum([h.es[e][\"length\"] for e in x.path]), axis = 1)\n",
    "\n",
    "# add path in nx edge id\n",
    "mygaps[\"path_nx\"] = mygaps.apply(lambda x: \n",
    "                                 [tuple(sorted(literal_eval(h.es[edge][\"edge_id\"]))) for edge in x.path], \n",
    "                                 axis = 1)\n",
    "\n",
    "# add origin and destination nodes\n",
    "# (separate procedure for gaps with edgenumber (enr) == 1 vs. gaps with enr > 1)\n",
    "mygaps[\"enr\"] = mygaps.apply(lambda x: len(x.path), axis = 1)\n",
    "mygaps[\"o_nx\"] = None\n",
    "mygaps[\"d_nx\"] = None\n",
    "mygaps.loc[mygaps[\"enr\"]==1, \"o_nx\"] = mygaps[mygaps[\"enr\"] == 1].apply(lambda x: x.path_nx[0][0], axis = 1)\n",
    "mygaps.loc[mygaps[\"enr\"]==1, \"d_nx\"] = mygaps[mygaps[\"enr\"] == 1].apply(lambda x: x.path_nx[0][1], axis = 1)\n",
    "mygaps.loc[mygaps[\"enr\"]!=1, \"o_nx\"] = mygaps[mygaps[\"enr\"]!=1].apply(lambda x: set(x.path_nx[0]).difference(x.path_nx[1]).pop(), axis = 1)\n",
    "mygaps.loc[mygaps[\"enr\"]!=1, \"d_nx\"] = mygaps[mygaps[\"enr\"]!=1].apply(lambda x: set(x.path_nx[-1]).difference(x.path_nx[-2]).pop(), axis = 1)\n",
    "mygaps.drop(columns = \"enr\", inplace = True)\n",
    "\n",
    "# add coordinates for  plotting\n",
    "mygaps[\"gapcoord\"] = mygaps.apply(lambda x: get_path_coords(x.path_nx, ced), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [[(5118125.883698401, 783143.0910435098), (511...\n",
      "1    [[(5118125.883698401, 783143.0910435098), (511...\n",
      "2    [[(5118125.883698401, 783143.0910435098), (511...\n",
      "3    [[(5118125.883698401, 783143.0910435098), (511...\n",
      "4    [[(5118125.883698401, 783143.0910435098), (511...\n",
      "Name: gapcoord, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(mygaps[\"gapcoord\"].head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
