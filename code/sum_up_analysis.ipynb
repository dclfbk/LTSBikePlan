{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict high risk areas \n",
    "\n",
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from shapely.geometry import Point\n",
    "from geopandas.tools import sjoin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, confusion_matrix, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import shap\n",
    "import os\n",
    "\n",
    "base_path = \"/Users/leonardo/Desktop/Tesi/LTSBikePlan/images\"\n",
    "city_name = \"Trento\"\n",
    "\n",
    "# Create the path for the new folder\n",
    "city_folder_path = os.path.join(base_path, city_name)\n",
    "\n",
    "# Load\n",
    "hex_totalscore = gpd.read_file('../data/hex_totalscore.shp')\n",
    "centrality_measures = pd.read_csv('../data/network_centrality_measures.csv')\n",
    "accidents = gpd.read_file('../data/accidents_trento.geojson')\n",
    "hex_totalscore.drop(columns=['index'], inplace=True)\n",
    "# Set 'geometry' as the geometry column\n",
    "hex_totalscore = hex_totalscore.set_geometry('geometry')\n",
    "hex_totalscore = hex_totalscore.set_crs(\"EPSG:25832\", allow_override=True) \n",
    "\n",
    "# Convert 'coordinates' column to Shapely Point objects\n",
    "def create_point(row):\n",
    "    x, y = map(float, row['coordinates'].strip('()').split(', '))\n",
    "    return Point(x, y)\n",
    "centrality_measures['geometry'] = centrality_measures.apply(create_point, axis=1)\n",
    "centrality_gdf = gpd.GeoDataFrame(centrality_measures, geometry='geometry')\n",
    "centrality_gdf = centrality_gdf.set_crs(\"EPSG:25832\", allow_override=True) \n",
    "\n",
    "# Filter accidents for years >= 2018\n",
    "accidents_filtered = accidents[accidents['anno'] >= 2018].copy()\n",
    "accidents_filtered['anno'] = accidents_filtered['anno'].astype(int)\n",
    "accidents_simplified = gpd.GeoDataFrame(accidents_filtered[['anno', 'geometry']])\n",
    "\n",
    "joined = gpd.sjoin(hex_totalscore, accidents_simplified, how='left', predicate='contains')\n",
    "accident_counts = joined.groupby(joined.index).size()\n",
    "accident_counts_df = pd.DataFrame(accident_counts, columns=['accidents_count'])\n",
    "hex_totalscore_with_accidents = hex_totalscore.merge(accident_counts_df, left_index=True, right_index=True, how='left')\n",
    "hex_totalscore_with_accidents['accidents_count'] = hex_totalscore_with_accidents['accidents_count'].fillna(0).astype(int)\n",
    "\n",
    "#print(hex_totalscore_with_accidents['accidents_count'].value_counts())\n",
    "\n",
    "# This joins the centrality data to the hexagons they fall within\n",
    "joined_with_centrality = gpd.sjoin(hex_totalscore_with_accidents, centrality_gdf, how='left', predicate='contains')\n",
    "\n",
    "# Group by the hexagon index and calculate means\n",
    "centrality_means = joined_with_centrality.groupby(joined_with_centrality.index).agg({\n",
    "    'degree_centrality': 'mean',\n",
    "    'betweenness_centrality': 'mean',\n",
    "    'closeness_centrality': 'mean'\n",
    "})\n",
    "\n",
    "# Rename columns\n",
    "centrality_means.columns = ['degree_centrality_mean', 'betweenness_centrality_mean', 'closeness_centrality_mean']\n",
    "\n",
    "# Merge the centrality means back into the original hex_totalscore_with_accidents DataFrame\n",
    "hex_totalscore_with_centrality = hex_totalscore_with_accidents.merge(centrality_means, left_index=True, right_index=True, how='left')\n",
    "\n",
    "# Fill NaN values with 0 (assuming no centrality data in those hexagons)\n",
    "hex_totalscore_with_centrality.fillna(0, inplace=True)\n",
    "\n",
    "df = hex_totalscore_with_centrality.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'accidents_count' is the target variable. \n",
    "# You might want to categorize this into 'high-risk' (1) and 'low-risk' (0) based on a threshold.\n",
    "\n",
    "# Define the threshold for high-risk and low-risk categorization\n",
    "percentile_threshold = 85  # Top 15%\n",
    "threshold = df['accidents_count'].quantile(percentile_threshold / 100)  \n",
    "df['risk_category'] = (df['accidents_count'] > threshold).astype(int)\n",
    "\n",
    "# Selecting features for the model\n",
    "features = ['total_scor', 'degree_centrality_mean', 'betweenness_centrality_mean', 'closeness_centrality_mean']\n",
    "X = df[features]\n",
    "y = df['risk_category']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Addressing Data Imbalance with Borderline-SMOTE\n",
    "smote_methods = {\n",
    "    'SMOTE': SMOTE(random_state=42),\n",
    "    'ADASYN': ADASYN(random_state=42),\n",
    "    'BorderlineSMOTE': BorderlineSMOTE(random_state=42),\n",
    "    'SMOTEENN': SMOTEENN(random_state=42)  \n",
    "}\n",
    "\n",
    "# Adjusting class weights for models\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "weights = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'logreg': {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'solver': ['lbfgs', 'liblinear']\n",
    "    },\n",
    "    'rfc': {\n",
    "        'n_estimators': [10, 50, 100, 200],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    },\n",
    "    'svm': {\n",
    "        'C': [0.01, 0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf', 'poly']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize models with balanced class weights\n",
    "models = {\n",
    "    'logreg': LogisticRegression(class_weight=weights),\n",
    "    'rfc': RandomForestClassifier(class_weight=weights),\n",
    "    'svm': SVC(class_weight=weights, probability=True)\n",
    "}\n",
    "\n",
    "# Applying GridSearchCV for hyperparameter tuning with cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for smote_key in smote_methods.keys():\n",
    "    X_train_smote, y_train_smote = smote_methods[smote_key].fit_resample(X_train, y_train)\n",
    "    print(f\"Using {smote_key} resampling technique:\")\n",
    "    for model_key in models.keys():\n",
    "        grid_search = GridSearchCV(models[model_key], param_grid[model_key], cv=skf, scoring='roc_auc')\n",
    "        grid_search.fit(X_train_smote, y_train_smote)\n",
    "        models[model_key] = grid_search.best_estimator_\n",
    "        print(f\"Best parameters for {model_key}: {grid_search.best_params_}\")\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    predictions = model.predict(X_test)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, predictions))\n",
    "    precision, recall, _ = precision_recall_curve(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    auc_score = auc(recall, precision)\n",
    "    roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    print(\"Precision-Recall AUC:\", auc_score)\n",
    "    print(\"ROC-AUC Score:\", roc_auc)\n",
    "\n",
    "# Evaluate models\n",
    "for model_key in models.keys():\n",
    "    print(f\"Evaluating {model_key}:\")\n",
    "    evaluate_model(models[model_key], X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the differences in precision and recall across models, the choice of the best model may depend on what is more critical for your application: reducing false positives or false negatives. If it's more important to not miss high-risk areas, logreg might be preferable despite its lower precision. If a balance is needed, rfc seems a better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrain on the full dataset\n",
    "\n",
    "# Re-running the Random Forest Classifier on the full dataset\n",
    "best_params_rfc = {'max_features': 'sqrt', 'n_estimators': 100}  # Use the best parameters from GridSearchCV\n",
    "\n",
    "# Initialize the Random Forest model with the best parameters\n",
    "rfc_model = RandomForestClassifier(max_features=best_params_rfc['max_features'], \n",
    "                                   n_estimators=best_params_rfc['n_estimators'],\n",
    "                                   class_weight=weights, random_state=42)\n",
    "\n",
    "# Since you are running the model on the full dataset, use the SMOTE method that performed best\n",
    "smote_method = SMOTE(random_state=42)  # Replace with the SMOTE method that worked best for you\n",
    "X_train_smote, y_train_smote = smote_method.fit_resample(X_train, y_train)\n",
    "\n",
    "# Fit the model\n",
    "rfc_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predictions and Evaluation\n",
    "predictions = rfc_model.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, predictions))\n",
    "precision, recall, _ = precision_recall_curve(y_test, rfc_model.predict_proba(X_test)[:, 1])\n",
    "auc_score = auc(recall, precision)\n",
    "roc_auc = roc_auc_score(y_test, rfc_model.predict_proba(X_test)[:, 1])\n",
    "print(\"Precision-Recall AUC:\", auc_score)\n",
    "print(\"ROC-AUC Score:\", roc_auc)\n",
    "\n",
    "# Using SHAP for model interpretation\n",
    "explainer = shap.TreeExplainer(rfc_model)\n",
    "shap_values = explainer.shap_values(X_train_smote)\n",
    "\n",
    "# Plot summary\n",
    "shap.summary_plot(shap_values, X_train_smote, feature_names=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the risk categories for the entire dataset\n",
    "df['predicted_risk_category'] = rfc_model.predict(scaler.transform(df[features]))\n",
    "\n",
    "# Merging the predictions back into the original geospatial dataset\n",
    "hex_totalscore_with_predictions = hex_totalscore.merge(df[['predicted_risk_category']], left_index=True, right_index=True, how='left')\n",
    "\n",
    "# Fill NaN values for areas that were not included in the prediction (if any)\n",
    "hex_totalscore_with_predictions['predicted_risk_category'].fillna(0, inplace=True)\n",
    "\n",
    "hex_totalscore_with_predictions = hex_totalscore_with_predictions.to_crs(epsg=4326)\n",
    "\n",
    "\n",
    "import folium\n",
    "\n",
    "# Convert your geodataframe to GeoJSON format\n",
    "hex_geojson = hex_totalscore_with_predictions.to_json()\n",
    "\n",
    "# Create a base map centered around Trento\n",
    "m = folium.Map(location=[46.0667, 11.1333], zoom_start=12) \n",
    "\n",
    "# Add the GeoJSON overlay to the map\n",
    "folium.GeoJson(\n",
    "    hex_geojson,\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': 'red' if feature['properties']['predicted_risk_category'] == 1 else 'green',\n",
    "        'color': 'black',\n",
    "        'weight': 1,\n",
    "        'dashArray': '5, 5',\n",
    "        'fillOpacity': 0.5,\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "file_path = os.path.join(city_folder_path, 'risk_accidents_hexagon.html')\n",
    "\n",
    "# Assuming 'accident_map' is a Folium Map object\n",
    "m.save(file_path)\n",
    "\n",
    "# Display the map\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Spatial autocorrelation between gaps and accidents\n",
    "import json\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "from shapely.geometry import Point, LineString\n",
    "import matplotlib.pyplot as plt\n",
    "from esda.moran import Moran_Local\n",
    "import libpysal as lps\n",
    "import osmnx as ox\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.packages import importr\n",
    "from geopandas.tools import sjoin_nearest\n",
    "from libpysal.weights import Queen\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from folium.map import FeatureGroup\n",
    "\n",
    "# Step 1: Load the Data\n",
    "\n",
    "with open('../data/filtered_gaps.json', 'r') as file:\n",
    "    data_read = json.load(file)\n",
    "\n",
    "# Convert the keys back to tuples (if necessary)\n",
    "filtered_gaps = {tuple(map(int, k[1:-1].split(', '))): v for k, v in data_read.items()}\n",
    "\n",
    "filepath = \"/Users/leonardo/Desktop/Tesi/LTSBikePlan/data/Trento_lts.graphml\"\n",
    "G_lts = ox.load_graphml(filepath)\n",
    "G_lts = ox.project_graph(G_lts, to_crs='EPSG:4326')\n",
    "isolated_nodes = list(nx.isolates(G_lts))\n",
    "G_lts.remove_nodes_from(isolated_nodes)\n",
    "\n",
    "edges = ox.graph_to_gdfs(G_lts, nodes=False)\n",
    "\n",
    "high_stress_lines = []\n",
    "for key, path in filtered_gaps.items():\n",
    "    points = []\n",
    "    for node in path:\n",
    "        # Check if the node exists in the graph and has the required data\n",
    "        if node in G_lts.nodes and 'x' in G_lts.nodes[node] and 'y' in G_lts.nodes[node]:\n",
    "            x = G_lts.nodes[node]['x']\n",
    "            y = G_lts.nodes[node]['y']\n",
    "            points.append(Point(x, y))\n",
    "        else:\n",
    "            print(f\"Node {node} data is missing or incomplete.\")\n",
    "    if points:\n",
    "        line = LineString(points)\n",
    "        high_stress_lines.append(line)\n",
    "\n",
    "high_stress_gdf = gpd.GeoDataFrame(geometry=high_stress_lines, crs='EPSG:4326')\n",
    "high_stress_gdf = high_stress_gdf.to_crs('EPSG:25832')\n",
    "accidents_near_lines = sjoin_nearest(accidents_simplified, high_stress_gdf, how='left')\n",
    "accidents_per_line = accidents_near_lines.groupby('index_right').size()\n",
    "\n",
    "lats = [G_lts.nodes[node]['y'] for node in G_lts.nodes]\n",
    "lngs = [G_lts.nodes[node]['x'] for node in G_lts.nodes]\n",
    "\n",
    "center_lat = sum(lats) / len(lats)\n",
    "center_lng = sum(lngs) / len(lngs)\n",
    "\n",
    "# Create a spatial weights matrix\n",
    "# w = Queen.from_dataframe(high_stress_gdf, use_index=True)\n",
    "\n",
    "# Identify islands\n",
    "# islands = w.islands\n",
    "\n",
    "# # Remove islands from the weights matrix and data\n",
    "# high_stress_gdf = high_stress_gdf.drop(islands)\n",
    "# accidents_per_line = accidents_per_line.drop(islands)\n",
    "\n",
    "#Recompute the spatial weights matrix\n",
    "w = Queen.from_dataframe(high_stress_gdf, use_index=True)\n",
    "w.transform = 'r'\n",
    "\n",
    "# Ensure that the index of accidents_per_line matches high_stress_gdf\n",
    "accidents_per_line = accidents_per_line.reindex(high_stress_gdf.index, fill_value=0)\n",
    "# Identify Disconnected Components\n",
    "components = w.component_labels\n",
    "unique_components = pd.unique(components)\n",
    "results = []\n",
    "for comp in unique_components:\n",
    "    comp_indexes = high_stress_gdf.index[components == comp]\n",
    "    comp_high_stress_gdf = high_stress_gdf.loc[comp_indexes]\n",
    "    comp_data = accidents_per_line.loc[comp_indexes]\n",
    "    if len(comp_data) > 2 and comp_data.var() != 0:\n",
    "        comp_weights = Queen.from_dataframe(comp_high_stress_gdf, use_index=True)\n",
    "        comp_weights.transform = 'r'\n",
    "        local_moran_comp = Moran_Local(comp_data, comp_weights)\n",
    "\n",
    "        for idx, (p_value, z_score, quadrant) in enumerate(zip(local_moran_comp.p_sim, local_moran_comp.z_sim, local_moran_comp.q)):\n",
    "            results.append({'Component': comp, 'Segment': comp_indexes[idx], 'P-value': p_value, 'Z-score': z_score, 'Quadrant': quadrant})\n",
    "    else:\n",
    "        print(f\"Component {comp} skipped due to insufficient data points or no variation.\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "high_stress_gdf = high_stress_gdf.merge(results_df, left_on=high_stress_gdf.index, right_on='Segment')\n",
    "high_stress_gdf = high_stress_gdf.to_crs('EPSG:4326')\n",
    "\n",
    "# Quadrants colors\n",
    "quadrant_colors = {\n",
    "    1: 'darkred',\n",
    "    2: 'yellow',\n",
    "    3: 'orange',\n",
    "    4: 'red'\n",
    "}\n",
    "\n",
    "m = folium.Map(location=[center_lat, center_lng], zoom_start=13)\n",
    "\n",
    "quadrant_1 = FeatureGroup(name='Quadrant 1: High-high').add_to(m)\n",
    "quadrant_2 = FeatureGroup(name='Quadrant 2: Low-low').add_to(m)\n",
    "quadrant_3 = FeatureGroup(name='Quadrant 3: Low-high').add_to(m)\n",
    "quadrant_4 = FeatureGroup(name='Quadrant 4: High-low').add_to(m)\n",
    "\n",
    "for _, row in high_stress_gdf.iterrows():\n",
    "    color = quadrant_colors.get(row['Quadrant'], 'blue')\n",
    "    line = folium.PolyLine(locations=[(y, x) for x, y in row['geometry'].coords], color=color, weight=3)\n",
    "    if row['Quadrant'] == 1:\n",
    "        line.add_to(quadrant_1)\n",
    "    elif row['Quadrant'] == 2:\n",
    "        line.add_to(quadrant_2)\n",
    "    elif row['Quadrant'] == 3:\n",
    "        line.add_to(quadrant_3)\n",
    "    elif row['Quadrant'] == 4:\n",
    "        line.add_to(quadrant_4)\n",
    "\n",
    "quadrant_1.add_to(m)\n",
    "quadrant_2.add_to(m)\n",
    "quadrant_3.add_to(m)\n",
    "quadrant_4.add_to(m)\n",
    "\n",
    "legend_html = '''\n",
    "<div style=\"position: fixed; \n",
    "     top: 10px; left: 50px; width: 230px; height: 120px; \n",
    "     border:2px solid grey; z-index:9999; font-size:14px; background: white;\n",
    "     padding: 5px;\">\n",
    "     <b>Quadrants - Risk of Accidents</b> <br>\n",
    "     &nbsp; Quadrant 1: <i style=\"background:darkred;width:12px;height:12px;display:inline-block;\"></i> High-high <br>\n",
    "     &nbsp; Quadrant 2: <i style=\"background:yellow;width:12px;height:12px;display:inline-block;\"></i> Low-low <br>\n",
    "     &nbsp; Quadrant 3: <i style=\"background:orange;width:12px;height:12px;display:inline-block;\"></i> Low-high <br>\n",
    "     &nbsp; Quadrant 4: <i style=\"background:red;width:12px;height:12px;display:inline-block;\"></i> High-low\n",
    "</div>\n",
    "'''\n",
    "\n",
    "from branca.element import Element\n",
    "\n",
    "legend = Element(legend_html)\n",
    "m.get_root().html.add_child(legend)\n",
    "\n",
    "\n",
    "folium.LayerControl(collapsed=False).add_to(m)\n",
    "\n",
    "file_path = os.path.join(city_folder_path, 'gap_quadrants.html')\n",
    "\n",
    "# Assuming 'accident_map' is a Folium Map object\n",
    "m.save(file_path)\n",
    "\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the code provides a detailed analysis of the spatial autocorrelation of accidents along different segments of the road network (referred to as 'high-stress lines') using Local Moran's I (LISA). Here's a breakdown of what the output signifies:\n",
    "\n",
    "### Understanding the Output for Each Segment\n",
    "- **P-value**: Indicates the probability that the observed spatial pattern (in this case, the concentration of accidents on a particular road segment) could have occurred by random chance. Lower p-values (typically below 0.05) suggest that the pattern is unlikely to be random and is statistically significant.\n",
    "- **Z-score**: Measures how many standard deviations an element is from the mean. A high positive or negative Z-score indicates a more pronounced clustering than expected under spatial randomness.\n",
    "- **Quadrant**: Indicates the type of spatial correlation:\n",
    "    - **Quadrant 1 (High-High)**: The segment has a high number of accidents and is surrounded by segments with high numbers of accidents.\n",
    "    - **Quadrant 2 (Low-Low)**: The segment has a low number of accidents and is surrounded by segments with low numbers of accidents.\n",
    "    - **Quadrant 3 (Low-High)**: The segment has a low number of accidents but is surrounded by segments with high numbers of accidents.\n",
    "    - **Quadrant 4 (High-Low)**: The segment has a high number of accidents but is surrounded by segments with low numbers of accidents.\n",
    "\n",
    "### Interpreting Results\n",
    "\n",
    "Significant Clusters of Accidents: Several segments have low p-values and high z-scores in Quadrants 1 and 3, indicating significant clusters of accidents. These could be areas of particular concern for road safety.\n",
    "- **High-High Clusters (Quadrant 1):** Segments in this quadrant suggest areas where accidents are consistently high, and neighboring segments also have high accident rates. These might be hotspots needing targeted interventions.\n",
    "- **Low-Low Clusters (Quadrant 3):** These segments indicate areas with fewer accidents, surrounded by areas with similarly low accident rates. These areas might be considered safer or less prone to accidents.\n",
    "- **High-Low and Low-High Clusters (Quadrants 2 and 4):** These segments indicate spatial outliers where the accident rate significantly differs from neighboring areas. They could point to unique local factors affecting road safety.\n",
    "\n",
    "### Overall Interpretation\n",
    "- The segments with low P-values and high absolute Z-scores indicate areas where the accident distribution is not random but shows a significant spatial pattern.\n",
    "- Quadrants help identify the nature of these patterns, whether they are clusters of high accident areas or isolated high accident segments surrounded by lower accident segments.\n",
    "\n",
    "### Additional Observations\n",
    "- **Disconnected Components**: The warning about disconnected components and islands in the weights matrix suggests that certain segments of the network do not connect to others, which could affect the global spatial autocorrelation analysis. These disconnected components might need separate analysis or consideration.\n",
    "- **Islands**: These are segments without neighboring segments in the spatial weights matrix, likely due to the way the road network is structured or due to data processing steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
