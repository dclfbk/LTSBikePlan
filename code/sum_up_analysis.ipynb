{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict high risk areas \n",
    "\n",
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from shapely.geometry import Point\n",
    "from geopandas.tools import sjoin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, confusion_matrix, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import shap\n",
    "import os\n",
    "\n",
    "base_path = \"/Users/leonardo/Desktop/Tesi/LTSBikePlan/images\"\n",
    "city_name = \"Trento\"\n",
    "\n",
    "# Create the path for the new folder\n",
    "city_folder_path = os.path.join(base_path, city_name)\n",
    "\n",
    "# Load\n",
    "hex_totalscore = gpd.read_file('../data/hex_totalscore.shp')\n",
    "centrality_measures = pd.read_csv('../data/network_centrality_measures.csv')\n",
    "accidents = gpd.read_file('../data/accidents_trento.geojson')\n",
    "hex_totalscore.drop(columns=['index'], inplace=True)\n",
    "# Set 'geometry' as the geometry column\n",
    "hex_totalscore = hex_totalscore.set_geometry('geometry')\n",
    "hex_totalscore = hex_totalscore.set_crs(\"EPSG:25832\", allow_override=True) \n",
    "\n",
    "# Convert 'coordinates' column to Shapely Point objects\n",
    "def create_point(row):\n",
    "    x, y = map(float, row['coordinates'].strip('()').split(', '))\n",
    "    return Point(x, y)\n",
    "centrality_measures['geometry'] = centrality_measures.apply(create_point, axis=1)\n",
    "centrality_gdf = gpd.GeoDataFrame(centrality_measures, geometry='geometry')\n",
    "centrality_gdf = centrality_gdf.set_crs(\"EPSG:25832\", allow_override=True) \n",
    "\n",
    "# Filter accidents for years >= 2018\n",
    "accidents_filtered = accidents[accidents['anno'] >= 2018].copy()\n",
    "accidents_filtered['anno'] = accidents_filtered['anno'].astype(int)\n",
    "accidents_simplified = gpd.GeoDataFrame(accidents_filtered[['anno', 'geometry']])\n",
    "\n",
    "joined = gpd.sjoin(hex_totalscore, accidents_simplified, how='left', predicate='contains')\n",
    "accident_counts = joined.groupby(joined.index).size()\n",
    "accident_counts_df = pd.DataFrame(accident_counts, columns=['accidents_count'])\n",
    "hex_totalscore_with_accidents = hex_totalscore.merge(accident_counts_df, left_index=True, right_index=True, how='left')\n",
    "hex_totalscore_with_accidents['accidents_count'] = hex_totalscore_with_accidents['accidents_count'].fillna(0).astype(int)\n",
    "\n",
    "#print(hex_totalscore_with_accidents['accidents_count'].value_counts())\n",
    "\n",
    "# This joins the centrality data to the hexagons they fall within\n",
    "joined_with_centrality = gpd.sjoin(hex_totalscore_with_accidents, centrality_gdf, how='left', predicate='contains')\n",
    "\n",
    "# Group by the hexagon index and calculate means\n",
    "centrality_means = joined_with_centrality.groupby(joined_with_centrality.index).agg({\n",
    "    'degree_centrality': 'mean',\n",
    "    'betweenness_centrality': 'mean',\n",
    "    'closeness_centrality': 'mean'\n",
    "})\n",
    "\n",
    "# Rename columns\n",
    "centrality_means.columns = ['degree_centrality_mean', 'betweenness_centrality_mean', 'closeness_centrality_mean']\n",
    "\n",
    "# Merge the centrality means back into the original hex_totalscore_with_accidents DataFrame\n",
    "hex_totalscore_with_centrality = hex_totalscore_with_accidents.merge(centrality_means, left_index=True, right_index=True, how='left')\n",
    "\n",
    "# Fill NaN values with 0 (assuming no centrality data in those hexagons)\n",
    "hex_totalscore_with_centrality.fillna(0, inplace=True)\n",
    "\n",
    "df = hex_totalscore_with_centrality.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'accidents_count' is the target variable. \n",
    "# You might want to categorize this into 'high-risk' (1) and 'low-risk' (0) based on a threshold.\n",
    "\n",
    "# Define the threshold for high-risk and low-risk categorization\n",
    "percentile_threshold = 85  # Top 15%\n",
    "threshold = df['accidents_count'].quantile(percentile_threshold / 100)  \n",
    "df['risk_category'] = (df['accidents_count'] > threshold).astype(int)\n",
    "\n",
    "# Selecting features for the model\n",
    "features = ['total_scor', 'degree_centrality_mean', 'betweenness_centrality_mean', 'closeness_centrality_mean']\n",
    "X = df[features]\n",
    "y = df['risk_category']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Addressing Data Imbalance \n",
    "smote_methods = {\n",
    "    'SMOTE': SMOTE(random_state=42),\n",
    "    'ADASYN': ADASYN(random_state=42),\n",
    "    'BorderlineSMOTE': BorderlineSMOTE(random_state=42),\n",
    "    'SMOTEENN': SMOTEENN(random_state=42)  \n",
    "}\n",
    "\n",
    "# Adjusting class weights for models\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "weights = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'logreg': {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'solver': ['lbfgs', 'liblinear']\n",
    "    },\n",
    "    'rfc': {\n",
    "        'n_estimators': [10, 50, 100, 200],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    },\n",
    "    'svm': {\n",
    "        'C': [0.01, 0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf', 'poly']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize models with balanced class weights\n",
    "models = {\n",
    "    'logreg': LogisticRegression(class_weight=weights),\n",
    "    'rfc': RandomForestClassifier(class_weight=weights),\n",
    "    'svm': SVC(class_weight=weights, probability=True)\n",
    "}\n",
    "\n",
    "# Applying GridSearchCV for hyperparameter tuning with cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for smote_key in smote_methods.keys():\n",
    "    X_train_smote, y_train_smote = smote_methods[smote_key].fit_resample(X_train, y_train)\n",
    "    print(f\"Using {smote_key} resampling technique:\")\n",
    "    for model_key in models.keys():\n",
    "        grid_search = GridSearchCV(models[model_key], param_grid[model_key], cv=skf, scoring='roc_auc')\n",
    "        grid_search.fit(X_train_smote, y_train_smote)\n",
    "        models[model_key] = grid_search.best_estimator_\n",
    "        print(f\"Best parameters for {model_key}: {grid_search.best_params_}\")\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    predictions = model.predict(X_test)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, predictions))\n",
    "    precision, recall, _ = precision_recall_curve(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    auc_score = auc(recall, precision)\n",
    "    roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    print(\"Precision-Recall AUC:\", auc_score)\n",
    "    print(\"ROC-AUC Score:\", roc_auc)\n",
    "\n",
    "# Evaluate models\n",
    "for model_key in models.keys():\n",
    "    print(f\"Evaluating {model_key}:\")\n",
    "    evaluate_model(models[model_key], X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the differences in precision and recall across models, the choice of the best model may depend on what is more critical for your application: reducing false positives or false negatives. If it's more important to not miss high-risk areas, logreg might be preferable despite its lower precision. If a balance is needed, rfc seems a better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrain on the full dataset\n",
    "\n",
    "# Re-running the Random Forest Classifier on the full dataset\n",
    "best_params_rfc = {'max_features': 'sqrt', 'n_estimators': 100}  # Use the best parameters from GridSearchCV\n",
    "\n",
    "# Initialize the Random Forest model with the best parameters\n",
    "rfc_model = RandomForestClassifier(max_features=best_params_rfc['max_features'], \n",
    "                                   n_estimators=best_params_rfc['n_estimators'],\n",
    "                                   class_weight=weights, random_state=42)\n",
    "\n",
    "# Since you are running the model on the full dataset, use the SMOTE method that performed best\n",
    "smote_method = SMOTE(random_state=42)  # Replace with the SMOTE method that worked best for you\n",
    "X_train_smote, y_train_smote = smote_method.fit_resample(X_train, y_train)\n",
    "\n",
    "# Fit the model\n",
    "rfc_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predictions and Evaluation\n",
    "predictions = rfc_model.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, predictions))\n",
    "precision, recall, _ = precision_recall_curve(y_test, rfc_model.predict_proba(X_test)[:, 1])\n",
    "auc_score = auc(recall, precision)\n",
    "roc_auc = roc_auc_score(y_test, rfc_model.predict_proba(X_test)[:, 1])\n",
    "print(\"Precision-Recall AUC:\", auc_score)\n",
    "print(\"ROC-AUC Score:\", roc_auc)\n",
    "\n",
    "# Using SHAP for model interpretation\n",
    "explainer = shap.TreeExplainer(rfc_model)\n",
    "shap_values = explainer.shap_values(X_train_smote)\n",
    "\n",
    "# Plot summary\n",
    "shap.summary_plot(shap_values, X_train_smote, feature_names=features)\n",
    "\n",
    "# Saving the plot to the specified path\n",
    "file_path = os.path.join(city_folder_path, \"shap_tn.png\")\n",
    "plt.savefig(file_path, bbox_inches='tight')\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming X_test and y_test are already defined and the rfc_model is trained\n",
    "\n",
    "# Calculate the ROC curve and AUC for the model\n",
    "fpr, tpr, thresholds = roc_curve(y_test, rfc_model.predict_proba(X_test)[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) - Random Forest Classifier')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the risk categories for the entire dataset\n",
    "df['predicted_risk_category'] = rfc_model.predict(scaler.transform(df[features]))\n",
    "\n",
    "# Merging the predictions back into the original geospatial dataset\n",
    "hex_totalscore_with_predictions = hex_totalscore.merge(df[['predicted_risk_category']], left_index=True, right_index=True, how='left')\n",
    "\n",
    "# Fill NaN values for areas that were not included in the prediction (if any)\n",
    "hex_totalscore_with_predictions['predicted_risk_category'].fillna(0, inplace=True)\n",
    "\n",
    "hex_totalscore_with_predictions = hex_totalscore_with_predictions.to_crs(epsg=4326)\n",
    "\n",
    "# Count the number of high risk and low risk areas\n",
    "risk_counts = hex_totalscore_with_predictions['predicted_risk_category'].value_counts()\n",
    "\n",
    "# Assuming that '1' represents high risk and '0' represents low risk\n",
    "high_risk_count = risk_counts.get(1, 0)\n",
    "low_risk_count = risk_counts.get(0, 0)\n",
    "\n",
    "print(f\"Number of high-risk areas: {high_risk_count}\")\n",
    "print(f\"Number of low-risk areas: {low_risk_count}\")\n",
    "\n",
    "\n",
    "import folium\n",
    "\n",
    "# Convert your geodataframe to GeoJSON format\n",
    "hex_geojson = hex_totalscore_with_predictions.to_json()\n",
    "\n",
    "# Create a base map centered around Trento\n",
    "m = folium.Map(location=[46.0667, 11.1333], zoom_start=12) \n",
    "\n",
    "# Add the GeoJSON overlay to the map\n",
    "folium.GeoJson(\n",
    "    hex_geojson,\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': 'red' if feature['properties']['predicted_risk_category'] == 1 else 'green',\n",
    "        'color': 'black',\n",
    "        'weight': 1,\n",
    "        'dashArray': '5, 5',\n",
    "        'fillOpacity': 0.5,\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "file_path = os.path.join(city_folder_path, 'risk_accidents_hexagon.html')\n",
    "\n",
    "# Assuming 'accident_map' is a Folium Map object\n",
    "m.save(file_path)\n",
    "\n",
    "# Display the map\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Spatial autocorrelation between gaps and accidents\n",
    "import json\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "from shapely.geometry import Point, LineString\n",
    "from esda.moran import Moran_Local\n",
    "import osmnx as ox\n",
    "from rpy2.robjects.packages import importr\n",
    "from geopandas.tools import sjoin_nearest\n",
    "from libpysal.weights import Queen\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from folium.map import FeatureGroup\n",
    "import folium\n",
    "\n",
    "# Step 1: Load the Data\n",
    "with open('../data/filtered_gaps.json', 'r') as file:\n",
    "    data_read = json.load(file)\n",
    "\n",
    "# Convert the keys back to tuples (if necessary)\n",
    "filtered_gaps = {tuple(map(int, k[1:-1].split(', '))): v for k, v in data_read.items()}\n",
    "\n",
    "filepath = \"/Users/leonardo/Desktop/Tesi/LTSBikePlan/data/Trento_lts.graphml\"\n",
    "G_lts = ox.load_graphml(filepath)\n",
    "G_lts = ox.project_graph(G_lts, to_crs='EPSG:4326')\n",
    "isolated_nodes = list(nx.isolates(G_lts))\n",
    "G_lts.remove_nodes_from(isolated_nodes)\n",
    "\n",
    "edges = ox.graph_to_gdfs(G_lts, nodes=False)\n",
    "\n",
    "high_stress_lines = []\n",
    "for key, path in filtered_gaps.items():\n",
    "    points = []\n",
    "    for node in path:\n",
    "        # Check if the node exists in the graph and has the required data\n",
    "        if node in G_lts.nodes and 'x' in G_lts.nodes[node] and 'y' in G_lts.nodes[node]:\n",
    "            x = G_lts.nodes[node]['x']\n",
    "            y = G_lts.nodes[node]['y']\n",
    "            points.append(Point(x, y))\n",
    "        else:\n",
    "            print(f\"Node {node} data is missing or incomplete.\")\n",
    "    if points:\n",
    "        line = LineString(points)\n",
    "        high_stress_lines.append(line)\n",
    "\n",
    "# def are_connected(line1, line2):\n",
    "#     \"\"\"\n",
    "#     Check if two LineString objects are connected.\n",
    "#     \"\"\"\n",
    "#     return line1.coords[-1] == line2.coords[0] or line1.coords[0] == line2.coords[-1]\n",
    "\n",
    "# Group connected segments\n",
    "# aggregated_segments = []\n",
    "# temp_segment = []\n",
    "\n",
    "# for line in high_stress_lines:\n",
    "#     if not temp_segment:\n",
    "#         temp_segment.append(line)\n",
    "#     else:\n",
    "#         if are_connected(temp_segment[-1], line):\n",
    "#             temp_segment.append(line)\n",
    "#         else:\n",
    "#             aggregated_segments.append(LineString([point for ls in temp_segment for point in ls.coords]))\n",
    "#             temp_segment = [line]\n",
    "\n",
    "# # Add the last segment if it exists\n",
    "# if temp_segment:\n",
    "#     aggregated_segments.append(LineString([point for ls in temp_segment for point in ls.coords]))\n",
    "\n",
    "high_stress_gdf = gpd.GeoDataFrame(geometry=high_stress_lines, crs='EPSG:4326')\n",
    "#high_stress_gdf = gpd.GeoDataFrame(geometry=aggregated_segments, crs='EPSG:4326')\n",
    "high_stress_gdf = high_stress_gdf.to_crs('EPSG:25832')\n",
    "\n",
    "accidents_near_lines = sjoin_nearest(accidents_simplified, high_stress_gdf, how='left')\n",
    "accidents_per_line = accidents_near_lines.groupby('index_right').size()\n",
    "\n",
    "lats = [G_lts.nodes[node]['y'] for node in G_lts.nodes]\n",
    "lngs = [G_lts.nodes[node]['x'] for node in G_lts.nodes]\n",
    "\n",
    "center_lat = sum(lats) / len(lats)\n",
    "center_lng = sum(lngs) / len(lngs)\n",
    "\n",
    "#Recompute the spatial weights matrix\n",
    "w = Queen.from_dataframe(high_stress_gdf, use_index=True)\n",
    "w.transform = 'r'\n",
    "\n",
    "# Ensure that the index of accidents_per_line matches high_stress_gdf\n",
    "accidents_per_line = accidents_per_line.reindex(high_stress_gdf.index, fill_value=0)\n",
    "# Identify Disconnected Components\n",
    "components = w.component_labels\n",
    "unique_components = pd.unique(components)\n",
    "results = []\n",
    "for comp in unique_components:\n",
    "    comp_indexes = high_stress_gdf.index[components == comp]\n",
    "    comp_high_stress_gdf = high_stress_gdf.loc[comp_indexes]\n",
    "    comp_data = accidents_per_line.loc[comp_indexes]\n",
    "    if len(comp_data) > 2 and comp_data.var() != 0:\n",
    "        comp_weights = Queen.from_dataframe(comp_high_stress_gdf, use_index=True)\n",
    "        comp_weights.transform = 'r'\n",
    "        local_moran_comp = Moran_Local(comp_data, comp_weights)\n",
    "\n",
    "        for idx, (p_value, z_score, quadrant) in enumerate(zip(local_moran_comp.p_sim, local_moran_comp.z_sim, local_moran_comp.q)):\n",
    "            results.append({'Component': comp, 'Segment': comp_indexes[idx], 'P-value': p_value, 'Z-score': z_score, 'Quadrant': quadrant})\n",
    "    else:\n",
    "        print(f\"Component {comp} skipped due to insufficient data points or no variation.\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "significant_results_df = results_df[results_df['P-value'] < 0.05]\n",
    "\n",
    "\n",
    "high_stress_gdf = high_stress_gdf.merge(significant_results_df, left_on=high_stress_gdf.index, right_on='Segment')\n",
    "high_stress_gdf = high_stress_gdf.to_crs('EPSG:4326')\n",
    "finale_gaps_gdf = gpd.read_file(\"../data/finale_gaps.geojson\")\n",
    "\n",
    "# Step 1: Perform spatial intersection\n",
    "intersections = gpd.sjoin(finale_gaps_gdf, high_stress_gdf, how='left', predicate='intersects')\n",
    "\n",
    "# Step 2: Assign 'Quadrant' value to gdf_finale\n",
    "# This will create a new DataFrame with 'geometry' from gdf_finale and 'Quadrant' from high_stress_gdf\n",
    "gdf_finale_with_quadrant = intersections[['geometry', 'Quadrant']]\n",
    "# Step 3: Handle multiple intersections (using the mode)\n",
    "gdf_finale_with_quadrant = gdf_finale_with_quadrant.groupby('geometry')['Quadrant'].agg(lambda x: x.mode()[0] if len(x) > 0 else None).reset_index()\n",
    "\n",
    "# Perform the groupby and aggregation to get the mode of the 'Quadrant'\n",
    "agg_result = gdf_finale_with_quadrant.groupby('geometry')['Quadrant'].agg(lambda x: x.mode()[0] if len(x) > 0 else None).reset_index()\n",
    "\n",
    "# Convert the 'geometry' back to a GeoSeries\n",
    "agg_result['geometry'] = agg_result['geometry'].apply(lambda x: LineString(x))\n",
    "\n",
    "# Create a new GeoDataFrame from the aggregated results\n",
    "gdf_finale_with_quadrant_geodf = gpd.GeoDataFrame(agg_result, geometry='geometry', crs=finale_gaps_gdf.crs)\n",
    "print(gdf_finale_with_quadrant_geodf)\n",
    "\n",
    "# Quadrants colors\n",
    "quadrant_colors = {\n",
    "    1: 'darkred',\n",
    "    2: 'yellow',\n",
    "    3: 'orange',\n",
    "    4: 'red'\n",
    "}\n",
    "\n",
    "m = folium.Map(location=[center_lat, center_lng], zoom_start=13)\n",
    "\n",
    "quadrant_1 = FeatureGroup(name='Quadrant 1: High-high').add_to(m)\n",
    "quadrant_2 = FeatureGroup(name='Quadrant 2: Low-low').add_to(m)\n",
    "quadrant_3 = FeatureGroup(name='Quadrant 3: Low-high').add_to(m)\n",
    "quadrant_4 = FeatureGroup(name='Quadrant 4: High-low').add_to(m)\n",
    "\n",
    "for _, row in gdf_finale_with_quadrant_geodf.iterrows():\n",
    "    color = quadrant_colors.get(row['Quadrant'], 'blue')\n",
    "    line = folium.PolyLine(locations=[(y, x) for x, y in row['geometry'].coords], color=color, weight=3)\n",
    "    if row['Quadrant'] == 1:\n",
    "        line.add_to(quadrant_1)\n",
    "    elif row['Quadrant'] == 2:\n",
    "        line.add_to(quadrant_2)\n",
    "    elif row['Quadrant'] == 3:\n",
    "        line.add_to(quadrant_3)\n",
    "    elif row['Quadrant'] == 4:\n",
    "        line.add_to(quadrant_4)\n",
    "\n",
    "quadrant_1.add_to(m)\n",
    "quadrant_2.add_to(m)\n",
    "quadrant_3.add_to(m)\n",
    "quadrant_4.add_to(m)\n",
    "\n",
    "legend_html = '''\n",
    "<div style=\"position: fixed; \n",
    "     top: 10px; left: 50px; width: 230px; height: 120px; \n",
    "     border:2px solid grey; z-index:9999; font-size:14px; background: white;\n",
    "     padding: 5px;\">\n",
    "     <b>Quadrants - Risk of Accidents</b> <br>\n",
    "     &nbsp; Quadrant 1: <i style=\"background:darkred;width:12px;height:12px;display:inline-block;\"></i> High-high <br>\n",
    "     &nbsp; Quadrant 2: <i style=\"background:yellow;width:12px;height:12px;display:inline-block;\"></i> Low-low <br>\n",
    "     &nbsp; Quadrant 3: <i style=\"background:orange;width:12px;height:12px;display:inline-block;\"></i> Low-high <br>\n",
    "     &nbsp; Quadrant 4: <i style=\"background:red;width:12px;height:12px;display:inline-block;\"></i> High-low\n",
    "</div>\n",
    "'''\n",
    "\n",
    "from branca.element import Element\n",
    "\n",
    "legend = Element(legend_html)\n",
    "m.get_root().html.add_child(legend)\n",
    "\n",
    "\n",
    "folium.LayerControl(collapsed=False).add_to(m)\n",
    "\n",
    "file_path = os.path.join(city_folder_path, 'gap_quadrants.html')\n",
    "\n",
    "# Assuming 'accident_map' is a Folium Map object\n",
    "m.save(file_path)\n",
    "\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esda.moran import Moran\n",
    "\n",
    "# Calculate the Global Moran's I\n",
    "global_moran = Moran(accidents_per_line, w)\n",
    "print(f\"Global Moran's I: {global_moran.I}\")\n",
    "print(f\"P-value: {global_moran.p_sim}\")\n",
    "\n",
    "mean_p_value = results_df['P-value'].mean()\n",
    "mean_z_score = results_df['Z-score'].mean()\n",
    "\n",
    "print(f\"Mean P-value: {mean_p_value}\")\n",
    "print(f\"Mean Z-score: {mean_z_score}\")\n",
    "\n",
    "# Calculate the proportion of significant Local Moran's I\n",
    "proportion_significant = (results_df['P-value'] < significance_threshold).mean()\n",
    "\n",
    "print(f\"Proportion of significant Local Moran's I: {proportion_significant}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the mode (the most frequently occurring value) in the case of multiple intersections is just one way to resolve situations where a LineString in gdf_finale intersects with multiple segments in high_stress_gdf, each potentially having different 'Quadrant' values. The mode provides a statistically significant way to assign a single 'Quadrant' value to a LineString in gdf_finale when it intersects with several segments in high_stress_gdf.\n",
    "\n",
    "The reason for choosing the mode is based on the assumption that the most common 'Quadrant' value among the intersected segments might be the most representative for the intersecting LineString in gdf_finale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "\n",
    "# Load data\n",
    "all_lts_df = pd.read_csv(\"/Users/leonardo/Desktop/Tesi/LTSBikePlan/data/Trento_all_lts.csv\")\n",
    "all_lts_df['geometry'] = all_lts_df['geometry'].apply(wkt.loads)\n",
    "all_lts = gpd.GeoDataFrame(all_lts_df, geometry='geometry', crs='EPSG:32632')\n",
    "\n",
    "# Convert to a suitable projected CRS (UTM zone 32N for Trento, Italy)\n",
    "projected_crs = 'EPSG:32632'\n",
    "all_lts = all_lts.to_crs(projected_crs)\n",
    "gdf_finale_with_quadrant_geodf = gdf_finale_with_quadrant_geodf.to_crs(projected_crs)\n",
    "\n",
    "# Perform the spatial join\n",
    "joined_gdf = gpd.sjoin_nearest(gdf_finale_with_quadrant_geodf, all_lts, how='left', distance_col=\"distance\")\n",
    "\n",
    "# Optional: Convert back to geographic CRS (EPSG:4326) if needed\n",
    "joined_gdf = joined_gdf.to_crs('EPSG:4326')\n",
    "\n",
    "# Filter the DataFrame for streets with Quadrant equal to 1,2,3,4\n",
    "streets_with_quadrant_1 = joined_gdf[joined_gdf['Quadrant'] == 1]\n",
    "streets_with_quadrant_2 = joined_gdf[joined_gdf['Quadrant'] == 2]\n",
    "streets_with_quadrant_3 = joined_gdf[joined_gdf['Quadrant'] == 3]\n",
    "streets_with_quadrant_4 = joined_gdf[joined_gdf['Quadrant'] == 4]\n",
    "\n",
    "# Extract unique street names\n",
    "unique_street_names_quadrant_1 = streets_with_quadrant_1['name'].drop_duplicates()\n",
    "unique_street_names_quadrant_2 = streets_with_quadrant_2['name'].drop_duplicates()\n",
    "unique_street_names_quadrant_3 = streets_with_quadrant_3['name'].drop_duplicates()\n",
    "unique_street_names_quadrant_4 = streets_with_quadrant_4['name'].drop_duplicates()\n",
    "\n",
    "# Convert to a list\n",
    "unique_street_names_list_1 = unique_street_names_quadrant_1.tolist()\n",
    "unique_street_names_list_2 = unique_street_names_quadrant_2.tolist()\n",
    "unique_street_names_list_3 = unique_street_names_quadrant_3.tolist()\n",
    "unique_street_names_list_4 = unique_street_names_quadrant_4.tolist()\n",
    "\n",
    "# Filter out 'nan' values and strip unnecessary characters\n",
    "cleaned_names_1 = [name.strip(\"['']\") for name in unique_street_names_list_1 if pd.notna(name)]\n",
    "cleaned_names_2 = [name.strip(\"['']\") for name in unique_street_names_list_2 if pd.notna(name)]\n",
    "cleaned_names_3 = [name.strip(\"['']\") for name in unique_street_names_list_3 if pd.notna(name)]\n",
    "cleaned_names_4 = [name.strip(\"['']\") for name in unique_street_names_list_4 if pd.notna(name)]\n",
    "\n",
    "# Display the cleaned list of names\n",
    "print(cleaned_names_1)\n",
    "print(cleaned_names_2)\n",
    "print(cleaned_names_3)\n",
    "print(cleaned_names_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Quadrant' and count the unique 'Component' in each group\n",
    "quadrant_component_count = significant_results_df.groupby('Quadrant')['Component'].nunique()\n",
    "\n",
    "# Convert the result to a DataFrame for easy presentation\n",
    "quadrant_component_count_df = quadrant_component_count.reset_index()\n",
    "quadrant_component_count_df.columns = ['Quadrant', 'Number of Components']\n",
    "\n",
    "# Display the result\n",
    "print(quadrant_component_count_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining significance threshold for P-value\n",
    "significance_threshold = 0.05\n",
    "\n",
    "# Summary Table\n",
    "summary = {\n",
    "    \"Component\": [],\n",
    "    \"Count per Quadrant\": [],\n",
    "    \"Average Z-Score per Quadrant\": [],\n",
    "    \"Proportion of Significant P-values\": []\n",
    "}\n",
    "\n",
    "for component in results_df['Component'].unique():\n",
    "    component_data =results_df[results_df['Component'] == component]\n",
    "    \n",
    "    # Count per Quadrant\n",
    "    quadrant_count = component_data['Quadrant'].value_counts().to_dict()\n",
    "    \n",
    "    # Average Z-Score per Quadrant\n",
    "    avg_z_score = component_data.groupby('Quadrant')['Z-score'].mean().to_dict()\n",
    "    \n",
    "    # Proportion of Significant P-values\n",
    "    significant_count = component_data[component_data['P-value'] < significance_threshold].shape[0]\n",
    "    proportion_significant = significant_count / component_data.shape[0]\n",
    "\n",
    "    summary[\"Component\"].append(component)\n",
    "    summary[\"Count per Quadrant\"].append(quadrant_count)\n",
    "    summary[\"Average Z-Score per Quadrant\"].append(avg_z_score)\n",
    "    summary[\"Proportion of Significant P-values\"].append(proportion_significant)\n",
    "\n",
    "# Converting the summary to a DataFrame\n",
    "summary_df = pd.DataFrame(summary)\n",
    "summary_df.set_index('Component', inplace=True)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the code provides a detailed analysis of the spatial autocorrelation of accidents along different segments of the road network (referred to as 'high-stress lines') using Local Moran's I (LISA). Here's a breakdown of what the output signifies:\n",
    "\n",
    "### Understanding the Output for Each Segment\n",
    "- **P-value**: Indicates the probability that the observed spatial pattern (in this case, the concentration of accidents on a particular road segment) could have occurred by random chance. Lower p-values (typically below 0.05) suggest that the pattern is unlikely to be random and is statistically significant.\n",
    "- **Z-score**: Measures how many standard deviations an element is from the mean. A high positive or negative Z-score indicates a more pronounced clustering than expected under spatial randomness.\n",
    "- **Quadrant**: Indicates the type of spatial correlation:\n",
    "    - **Quadrant 1 (High-High)**: The segment has a high number of accidents and is surrounded by segments with high numbers of accidents.\n",
    "    - **Quadrant 2 (Low-Low)**: The segment has a low number of accidents and is surrounded by segments with low numbers of accidents.\n",
    "    - **Quadrant 3 (Low-High)**: The segment has a low number of accidents but is surrounded by segments with high numbers of accidents.\n",
    "    - **Quadrant 4 (High-Low)**: The segment has a high number of accidents but is surrounded by segments with low numbers of accidents.\n",
    "\n",
    "### Interpreting Results\n",
    "\n",
    "Significant Clusters of Accidents: Several segments have low p-values and high z-scores in Quadrants 1 and 3, indicating significant clusters of accidents. These could be areas of particular concern for road safety.\n",
    "- **High-High Clusters (Quadrant 1):** Segments in this quadrant suggest areas where accidents are consistently high, and neighboring segments also have high accident rates. These might be hotspots needing targeted interventions.\n",
    "- **Low-Low Clusters (Quadrant 3):** These segments indicate areas with fewer accidents, surrounded by areas with similarly low accident rates. These areas might be considered safer or less prone to accidents.\n",
    "- **High-Low and Low-High Clusters (Quadrants 2 and 4):** These segments indicate spatial outliers where the accident rate significantly differs from neighboring areas. They could point to unique local factors affecting road safety.\n",
    "\n",
    "### Overall Interpretation\n",
    "- The segments with low P-values and high absolute Z-scores indicate areas where the accident distribution is not random but shows a significant spatial pattern.\n",
    "- Quadrants help identify the nature of these patterns, whether they are clusters of high accident areas or isolated high accident segments surrounded by lower accident segments.\n",
    "\n",
    "### Additional Observations\n",
    "- **Disconnected Components**: The warning about disconnected components and islands in the weights matrix suggests that certain segments of the network do not connect to others, which could affect the global spatial autocorrelation analysis. These disconnected components might need separate analysis or consideration.\n",
    "- **Islands**: These are segments without neighboring segments in the spatial weights matrix, likely due to the way the road network is structured or due to data processing steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
